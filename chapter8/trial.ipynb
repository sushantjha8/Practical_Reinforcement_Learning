{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import glob\n",
    "import numpy as np\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Prices = collections.namedtuple('Prices', field_names=['open', 'high', 'low', 'close', 'volume'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"D:\\Practical_Reinforcement_Learning\\chapter8\\data\\ch08-small-quotes\\YNDX_150101_151231.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar2rel(df,tolerance):\n",
    "    prev_vals = None\n",
    "    fix_open_price  = True\n",
    "    o, h, l, c, v = [], [], [], [], []\n",
    "    count_out = 0\n",
    "    count_filter = 0\n",
    "    count_fixed = 0\n",
    "    for row in df.itertuples():\n",
    "        val = (row._3,row._4,row._5,row._6,row._7)\n",
    "        po, ph, pl,pc,pv = val\n",
    "        if fix_open_price and prev_vals is not None:\n",
    "            ppo, pph, ppl, ppc, ppv = prev_vals\n",
    "            if abs(po - ppc) > 1e-8:\n",
    "                count_fixed += 1\n",
    "                po = ppc\n",
    "                pl = min(pl, po)\n",
    "                ph = max(ph, po)\n",
    "                count_out += 1\n",
    "        o.append(po)\n",
    "        c.append(pc)\n",
    "        h.append(ph)\n",
    "        l.append(pl)\n",
    "        v.append(pv)\n",
    "        prev_vals = val\n",
    "    prices=Prices(open=np.array(o, dtype=np.float32),\n",
    "                  high=np.array(h, dtype=np.float32),\n",
    "                  low=np.array(l, dtype=np.float32),\n",
    "                  close=np.array(c, dtype=np.float32),\n",
    "                  volume=np.array(v, dtype=np.float32))\n",
    "    return prices_to_relative(prices)\n",
    "\n",
    "def prices_to_relative(prices):\n",
    "    \"\"\"\n",
    "    Convert prices to relative in respect to open price\n",
    "    :param ochl: tuple with open, close, high, low\n",
    "    :return: tuple with open, rel_close, rel_high, rel_low\n",
    "    \"\"\"\n",
    "    assert isinstance(prices, Prices)\n",
    "    rh = (prices.high - prices.open) / prices.open\n",
    "    rl = (prices.low - prices.open) / prices.open\n",
    "    rc = (prices.close - prices.open) / prices.open\n",
    "    return Prices(open=prices.open, high=rh, low=rl, close=rc, volume=prices.volume)\n",
    "\n",
    "def preprocess(path):\n",
    "    df = pd.read_csv(os.path.abspath(train_path))\n",
    "\n",
    "    index = ['<OPEN>', \"<HIGH>\", \"<LOW>\",\"<CLOSE>\",\"<VOL>\"]\n",
    "    df[index] = df[index].astype(float)\n",
    "    df_normalized = (df - df.min()) / (df.max() - df.min())\n",
    "    # Define the tolerance value\n",
    "    tolerance = 1e-8\n",
    "\n",
    "    # Apply the lambda function to check if each value is within the tolerance of the first value\n",
    "    #result = df_normalized.applymap(lambda v: abs(v - df_normalized.iloc[0]) < tolerance)\n",
    "    return bar2rel(df_normalized,tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym.spaces\n",
    "from gym.utils import seeding\n",
    "from gym.envs.registration import EnvSpec\n",
    "import enum\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "DEFAULT_BARS_COUNT = 10\n",
    "DEFAULT_COMMISSION_PERC = 0.1\n",
    "\n",
    "\n",
    "\n",
    "class Actions(enum.Enum):\n",
    "    Skip = 0\n",
    "    Buy = 1\n",
    "    Close = 2\n",
    "\n",
    "class State:\n",
    "    def __init__(self, bars_count, commission_perc,\n",
    "                 reset_on_close, reward_on_close=True,\n",
    "                 volumes=True):\n",
    "        assert isinstance(bars_count, int)\n",
    "        assert bars_count > 0\n",
    "        assert isinstance(commission_perc, float)\n",
    "        assert commission_perc >= 0.0\n",
    "        assert isinstance(reset_on_close, bool)\n",
    "        assert isinstance(reward_on_close, bool)\n",
    "        self.bars_count = bars_count\n",
    "        self.commission_perc = commission_perc\n",
    "        self.reset_on_close = reset_on_close\n",
    "        self.reward_on_close = reward_on_close\n",
    "        self.volumes = volumes\n",
    "\n",
    "    def reset(self, prices, offset):\n",
    "        assert isinstance(prices, Prices)\n",
    "        assert offset >= self.bars_count-1\n",
    "        self.have_position = False\n",
    "        self.open_price = 0.0\n",
    "        self._prices = prices\n",
    "        self._offset = offset\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        # [h, l, c] * bars + position_flag + rel_profit\n",
    "        if self.volumes:\n",
    "            return 4 * self.bars_count + 1 + 1,\n",
    "        else:\n",
    "            return 3*self.bars_count + 1 + 1,\n",
    "\n",
    "    def encode(self):\n",
    "        \"\"\"\n",
    "        Convert current state into numpy array.\n",
    "        \"\"\"\n",
    "        res = np.ndarray(shape=self.shape, dtype=np.float32)\n",
    "        shift = 0\n",
    "        for bar_idx in range(-self.bars_count+1, 1):\n",
    "            ofs = self._offset + bar_idx\n",
    "            \n",
    "            res[shift] = self._prices.high[ofs]\n",
    "            shift += 1\n",
    "            res[shift] = self._prices.low[ofs]\n",
    "            shift += 1\n",
    "            res[shift] = self._prices.close[ofs]\n",
    "            shift += 1\n",
    "            if self.volumes:\n",
    "                res[shift] = self._prices.volume[ofs]\n",
    "                shift += 1\n",
    "            print(f\"\"\"state off set ofs {ofs}\\n and shape res as batch from offset {res.shape} \\n \n",
    "                    state_offset {self._offset} \\n bar_idx {bar_idx} \\n shift {shift} \\n res shift {res}\"\"\")\n",
    "        res[shift] = float(self.have_position)\n",
    "        shift += 1\n",
    "        if not self.have_position:\n",
    "            res[shift] = 0.0\n",
    "        else:\n",
    "            res[shift] = self._cur_close() / self.open_price - 1.0\n",
    "        print(f\"Final res shape {res.shape} shift {shift}\")\n",
    "        return res\n",
    "\n",
    "    def _cur_close(self):\n",
    "        \"\"\"\n",
    "        Calculate real close price for the current bar\n",
    "        \"\"\"\n",
    "        open = self._prices.open[self._offset]\n",
    "        rel_close = self._prices.close[self._offset]\n",
    "        return open * (1.0 + rel_close)\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Perform one step in our price, adjust offset, check for the end of prices\n",
    "        and handle position change\n",
    "        :param action:\n",
    "        :return: reward, done\n",
    "        \"\"\"\n",
    "        assert isinstance(action, Actions)\n",
    "        reward = 0.0\n",
    "        done = False\n",
    "        close = self._cur_close()\n",
    "        if action == Actions.Buy and not self.have_position:\n",
    "            self.have_position = True\n",
    "            self.open_price = close\n",
    "            reward -= self.commission_perc\n",
    "        elif action == Actions.Close and self.have_position:\n",
    "            reward -= self.commission_perc\n",
    "            done |= self.reset_on_close\n",
    "            if self.reward_on_close:\n",
    "                reward += 100.0 * (close / self.open_price - 1.0)\n",
    "            self.have_position = False\n",
    "            self.open_price = 0.0\n",
    "\n",
    "        self._offset += 1\n",
    "        prev_close = close\n",
    "        close = self._cur_close()\n",
    "        done |= self._offset >= self._prices.close.shape[0]-1\n",
    "\n",
    "        if self.have_position and not self.reward_on_close:\n",
    "            reward += 100.0 * (close / prev_close - 1.0)\n",
    "\n",
    "        return reward, done\n",
    "\n",
    "\n",
    "class StocksEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    #spec = EnvSpec(\"StocksEnv-v0\",entry_point=libs.envoiran.StocksEnv)\n",
    "\n",
    "    def __init__(self, prices: Prices, bars_count=DEFAULT_BARS_COUNT,\n",
    "                 commission=DEFAULT_COMMISSION_PERC,\n",
    "                 reset_on_close=True, state_1d=False,\n",
    "                 random_ofs_on_reset=True, reward_on_close=False,\n",
    "                 volumes=False):\n",
    "        self._prices = prices\n",
    "        self._state = State(\n",
    "            bars_count, commission, reset_on_close,\n",
    "            reward_on_close=reward_on_close, volumes=volumes)\n",
    "        self.action_space = gym.spaces.Discrete(n=len(Actions))\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=-np.inf, high=np.inf,\n",
    "            shape=self._state.shape, dtype=np.float32)\n",
    "        self.random_ofs_on_reset = random_ofs_on_reset\n",
    "        \n",
    "        #self.seed()\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed1 = seeding.np_random(seed)\n",
    "        seed2 = seeding.hash_seed(seed1 + 1) % 2 ** 31\n",
    "        return [seed1, seed2]\n",
    "    \n",
    "    def reset(self):\n",
    "        self._instrument = self.np_random.choice(\n",
    "            list(self._prices._fields))\n",
    "        if self._instrument is \"open\":\n",
    "            prices = self._prices.open\n",
    "        if self._instrument is \"close\":\n",
    "            prices = self._prices.close\n",
    "        if self._instrument is \"high\":\n",
    "            prices = self._prices.high\n",
    "        if self._instrument is \"low\":\n",
    "            prices = self._prices.low\n",
    "        else:\n",
    "            prices = self._prices.volume\n",
    "        bars = self._state.bars_count\n",
    "        if self.random_ofs_on_reset:\n",
    "            offset = self.np_random.choice(\n",
    "                prices.shape[0]-bars*10) + bars\n",
    "        else:\n",
    "            offset = bars\n",
    "        print(self._prices.low[offset],offset)\n",
    "        \n",
    "        # return P, offset\n",
    "        self._state.reset(self._prices, offset)\n",
    "        return self._state.encode()\n",
    "    def step(self, action_idx):\n",
    "        action = Actions(action_idx)\n",
    "        reward, done = self._state.step(action)\n",
    "        obs = self._state.encode()\n",
    "        info = {\n",
    "\n",
    "            \n",
    "                \"instrument\": self._instrument,\n",
    "                \"offset\": self._state._offset\n",
    "                }\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp=preprocess(train_path)\n",
    "env  = StocksEnv(rp, bars_count=10,\n",
    "                 commission=0.1,\n",
    "                 reset_on_close=True, state_1d=False,\n",
    "                 random_ofs_on_reset=True, reward_on_close=False,\n",
    "                 volumes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All done for env .\n",
    "lets setup model to get q value from our observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The provided class SimpleFFDQN is a neural network model defined using PyTorch. It represents a simple implementation of the Dueling Double Deep Q-Network (Dueling DQN) architecture.\n",
    "\n",
    "Here's what the forward function does:\n",
    "\n",
    "Value Stream (self.fc_val):\n",
    "\n",
    "The input x (which represents the state observation) is passed through a series of fully connected layers (nn.Linear) with ReLU activation functions (nn.ReLU).\n",
    "The output of the last linear layer is a single scalar value, representing the estimated value of the state (hence the name val). This value represents the expected return (or total future reward) that can be obtained from being in the given state.\n",
    "Advantage Stream (self.fc_adv):\n",
    "\n",
    "Similar to the value stream, the input x is passed through a series of fully connected layers with ReLU activation functions.\n",
    "The output of the last linear layer is a vector with actions_n elements, where each element represents the estimated advantage for each action available in the environment.\n",
    "Combining Value and Advantage Streams:\n",
    "\n",
    "The value stream and the advantage stream are combined to produce the final output of the network. This is done by adding the value estimates (val) to the advantages (adv) after centering the advantages by subtracting their mean (adv - adv.mean(dim=1, keepdim=True)). This step helps in stabilizing the learning process by ensuring that the network can learn relative advantages of different actions while still having a baseline value estimate for each state.\n",
    "The final output of the network is a tensor with actions_n elements, where each element represents the estimated Q-value for each action, considering both the state value and the advantages of each action.\n",
    "\n",
    "This architecture separates the representation of state values and action advantages, which is a key idea in Dueling DQN. It helps in more stable and efficient learning, especially in environments with a large number of actions or where the advantages of different actions can vary significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleFFDQN(nn.Module):\n",
    "    def __init__(self, obs_len, actions_n):\n",
    "        super(SimpleFFDQN, self).__init__()\n",
    "\n",
    "        self.fc_val = nn.Sequential(\n",
    "            nn.Linear(obs_len, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "        self.fc_adv = nn.Sequential(\n",
    "            nn.Linear(obs_len, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, actions_n)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        val = self.fc_val(x)\n",
    "        adv = self.fc_adv(x)\n",
    "        # The value stream and the advantage stream are combined to produce the final output of the network.\n",
    "        # This is done by adding the value estimates (val) to the advantages (adv) after centering the advantages by subtracting their\n",
    "        # mean (adv - adv.mean(dim=1, keepdim=True)). T\n",
    "        # This step helps in stabilizing the learning process by ensuring that the network can learn relative advantages of different actions\n",
    "        # while still having a baseline value estimate for each state.The final output of the network is a tensor with actions_n elements, \n",
    "        # where each element represents the estimated Q-value for each action, considering both the state value and the advantages of each action.\n",
    "        return val + (adv - adv.mean(dim=1, keepdim=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import warnings\n",
    "from typing import Iterable\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import ptan\n",
    "from ignite.engine import Engine\n",
    "from ignite.metrics import RunningAverage\n",
    "from ignite.contrib.handlers import TensorboardLogger\n",
    "from ignite.contrib.handlers import tensorboard_logger\n",
    "\n",
    "@torch.no_grad()\n",
    "def calc_values_of_states(states, net, device=\"cpu\"):\n",
    "    mean_vals = []\n",
    "    for batch in np.array_split(states, 64):\n",
    "        states_v = torch.tensor(batch).to(device)\n",
    "        action_values_v = net(states_v)\n",
    "        best_action_values_v = action_values_v.max(1)[0]\n",
    "        mean_vals.append(best_action_values_v.mean().item())\n",
    "    return np.mean(mean_vals)\n",
    "\n",
    "\n",
    "def unpack_batch(batch):\n",
    "    states, actions, rewards, dones, last_states = [], [], [], [], []\n",
    "    for exp in batch:\n",
    "        state = np.array(exp.state, copy=False)\n",
    "        states.append(state)\n",
    "        actions.append(exp.action)\n",
    "        rewards.append(exp.reward)\n",
    "        dones.append(exp.last_state is None)\n",
    "        if exp.last_state is None:\n",
    "            last_states.append(state)       # the result will be masked anyway\n",
    "        else:\n",
    "            last_states.append(np.array(exp.last_state, copy=False))\n",
    "    return np.array(states, copy=False), np.array(actions), np.array(rewards, dtype=np.float32), \\\n",
    "           np.array(dones, dtype=np.uint8), np.array(last_states, copy=False)\n",
    "\n",
    "\n",
    "def calc_loss(batch, net, tgt_net, gamma, device=\"cpu\"):\n",
    "    states, actions, rewards, dones, next_states = unpack_batch(batch)\n",
    "\n",
    "    states_v = torch.tensor(states).to(device)\n",
    "    next_states_v = torch.tensor(next_states).to(device)\n",
    "    actions_v = torch.tensor(actions).to(device)\n",
    "    rewards_v = torch.tensor(rewards).to(device)\n",
    "    done_mask = torch.BoolTensor(dones).to(device)\n",
    "\n",
    "    state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
    "    next_state_actions = net(next_states_v).max(1)[1]\n",
    "    next_state_values = tgt_net(next_states_v).gather(1, next_state_actions.unsqueeze(-1)).squeeze(-1)\n",
    "    next_state_values[done_mask] = 0.0\n",
    "\n",
    "    expected_state_action_values = next_state_values.detach() * gamma + rewards_v\n",
    "    return nn.MSELoss()(state_action_values, expected_state_action_values)\n",
    "\n",
    "\n",
    "def batch_generator(buffer: ptan.experience.ExperienceReplayBuffer,\n",
    "                    initial: int, batch_size: int):\n",
    "    buffer.populate(initial)\n",
    "    while True:\n",
    "        buffer.populate(1)\n",
    "        yield buffer.sample(batch_size)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ptan\n",
    "import pathlib\n",
    "import argparse\n",
    "import gym.wrappers\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from ignite.engine import Engine\n",
    "from ignite.contrib.handlers import tensorboard_logger\n",
    "# from ignite.handlers import tensorboard_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ignite.handlers.tensorboard_logger.OutputHandler"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorboard_logger.OutputHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"D:\\Practical_Reinforcement_Learning\\chapter8\\data\\ch08-small-quotes\\YNDX_150101_151231.csv\"\n",
    "val_path = \"D:\\Practical_Reinforcement_Learning\\chapter8\\data\\ch08-small-quotes\\YNDX_150101_151231.csv\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "BARS_COUNT = 10\n",
    "\n",
    "EPS_START = 1.0\n",
    "EPS_FINAL = 0.1\n",
    "EPS_STEPS = 1000000\n",
    "\n",
    "GAMMA = 0.99\n",
    "\n",
    "REPLAY_SIZE = 100000\n",
    "REPLAY_INITIAL = 10000\n",
    "REWARD_STEPS = 2\n",
    "LEARNING_RATE = 0.0001\n",
    "STATES_TO_EVALUATE = 1000\n",
    "\n",
    "tp= preprocess(train_path)\n",
    "env = StocksEnv( tp,bars_count=10,\n",
    "                 commission=0.1,\n",
    "                 reset_on_close=True, state_1d=False,\n",
    "                 random_ofs_on_reset=True, reward_on_close=False,\n",
    "                 volumes=True)\n",
    "# env = gym.wrappers.TimeLimit(env, max_episode_steps=1000)\n",
    "vp = preprocess(val_path)\n",
    "env_val = StocksEnv(vp, bars_count=10,\n",
    "                 commission=0.1,\n",
    "                 reset_on_close=True, state_1d=False,\n",
    "                 random_ofs_on_reset=True, reward_on_close=False,\n",
    "                 volumes=True)\n",
    "\n",
    "net = SimpleFFDQN(env.observation_space.shape[0],\n",
    "                            env.action_space.n).to(device)\n",
    "tgt_net = ptan.agent.TargetNet(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "class EpsilonTracker:\n",
    "    \"\"\"\n",
    "    Updates epsilon according to linear schedule\n",
    "    \"\"\"\n",
    "    def __init__(self, selector: ptan.actions.EpsilonGreedyActionSelector,\n",
    "                 eps_start: Union[int, float],\n",
    "                 eps_final: Union[int, float],\n",
    "                 eps_frames: int):\n",
    "        self.selector = selector\n",
    "        self.eps_start = eps_start\n",
    "        self.eps_final = eps_final\n",
    "        self.eps_frames = eps_frames\n",
    "        self.frame(0)\n",
    "\n",
    "    def frame(self, frame: int):\n",
    "        eps = self.eps_start - frame / self.eps_frames\n",
    "        self.selector.epsilon = max(self.eps_final, eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "selector = ptan.actions.EpsilonGreedyActionSelector(EPS_START)\n",
    "eps_tracker = EpsilonTracker(\n",
    "    selector, EPS_START, EPS_FINAL, EPS_STEPS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ptan.agent.DQNAgent(net, selector, device=device)\n",
    "exp_source = ptan.experience.ExperienceSourceFirstLast(\n",
    "    env, agent, GAMMA, steps_count=REWARD_STEPS)\n",
    "buffer = ptan.experience.ExperienceReplayBuffer(\n",
    "    exp_source, REPLAY_SIZE)\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = batch_generator(buffer,12,32)\n",
    "for experience in batch:\n",
    "    print(experience)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
