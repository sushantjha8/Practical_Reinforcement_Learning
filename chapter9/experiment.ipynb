{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import glob\n",
    "import numpy as np\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Prices = collections.namedtuple('Prices', field_names=['open', 'high', 'low', 'close', 'volume'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"D:\\Practical_Reinforcement_Learning\\chapter9\\data\\ch08-small-quotes\\YNDX_150101_151231.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar2rel(df,tolerance):\n",
    "    prev_vals = None\n",
    "    fix_open_price  = True\n",
    "    o, h, l, c, v = [], [], [], [], []\n",
    "    count_out = 0\n",
    "    count_filter = 0\n",
    "    count_fixed = 0\n",
    "    for row in df.itertuples():\n",
    "        val = (row._3,row._4,row._5,row._6,row._7)\n",
    "        po, ph, pl,pc,pv = val\n",
    "        if fix_open_price and prev_vals is not None:\n",
    "            ppo, pph, ppl, ppc, ppv = prev_vals\n",
    "            if abs(po - ppc) > 1e-8:\n",
    "                count_fixed += 1\n",
    "                po = ppc\n",
    "                pl = min(pl, po)\n",
    "                ph = max(ph, po)\n",
    "                count_out += 1\n",
    "        o.append(po)\n",
    "        c.append(pc)\n",
    "        h.append(ph)\n",
    "        l.append(pl)\n",
    "        v.append(pv)\n",
    "        prev_vals = val\n",
    "    prices=Prices(open=np.array(o, dtype=np.float32),\n",
    "                  high=np.array(h, dtype=np.float32),\n",
    "                  low=np.array(l, dtype=np.float32),\n",
    "                  close=np.array(c, dtype=np.float32),\n",
    "                  volume=np.array(v, dtype=np.float32))\n",
    "    return prices_to_relative(prices)\n",
    "\n",
    "def prices_to_relative(prices):\n",
    "    \"\"\"\n",
    "    Convert prices to relative in respect to open price\n",
    "    :param ochl: tuple with open, close, high, low\n",
    "    :return: tuple with open, rel_close, rel_high, rel_low\n",
    "    \"\"\"\n",
    "    assert isinstance(prices, Prices)\n",
    "    rh = (prices.high - prices.open) / prices.open\n",
    "    rl = (prices.low - prices.open) / prices.open\n",
    "    rc = (prices.close - prices.open) / prices.open\n",
    "    return Prices(open=prices.open, high=rh, low=rl, close=rc, volume=prices.volume)\n",
    "\n",
    "def preprocess(path):\n",
    "    df = pd.read_csv(os.path.abspath(train_path))\n",
    "\n",
    "    index = ['<OPEN>', \"<HIGH>\", \"<LOW>\",\"<CLOSE>\",\"<VOL>\"]\n",
    "    df[index] = df[index].astype(float)\n",
    "    df_normalized = (df - df.min()) / (df.max() - df.min())\n",
    "    # Define the tolerance value\n",
    "    tolerance = 1e-8\n",
    "\n",
    "    # Apply the lambda function to check if each value is within the tolerance of the first value\n",
    "    df_normalized.applymap(lambda v: abs(v - df_normalized.iloc[0]) < tolerance)\n",
    "    return bar2rel(df_normalized,tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym.spaces\n",
    "from gym.utils import seeding\n",
    "from gym.envs.registration import EnvSpec\n",
    "import enum\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "DEFAULT_BARS_COUNT = 10\n",
    "DEFAULT_COMMISSION_PERC = 0.1\n",
    "\n",
    "\n",
    "\n",
    "class Actions(enum.Enum):\n",
    "    Skip = 0\n",
    "    Buy = 1\n",
    "    Close = 2\n",
    "\n",
    "class State:\n",
    "    def __init__(self, bars_count, commission_perc,\n",
    "                 reset_on_close, reward_on_close=True,\n",
    "                 volumes=True):\n",
    "        assert isinstance(bars_count, int)\n",
    "        assert bars_count > 0\n",
    "        assert isinstance(commission_perc, float)\n",
    "        assert commission_perc >= 0.0\n",
    "        assert isinstance(reset_on_close, bool)\n",
    "        assert isinstance(reward_on_close, bool)\n",
    "        self.bars_count = bars_count\n",
    "        self.commission_perc = commission_perc\n",
    "        self.reset_on_close = reset_on_close\n",
    "        self.reward_on_close = reward_on_close\n",
    "        self.volumes = volumes\n",
    "\n",
    "    def reset(self, prices, offset):\n",
    "        assert isinstance(prices, Prices)\n",
    "        assert offset >= self.bars_count-1\n",
    "        self.have_position = False\n",
    "        self.open_price = 0.0\n",
    "        self._prices = prices\n",
    "        self._offset = offset\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        # [h, l, c] * bars + position_flag + rel_profit\n",
    "        if self.volumes:\n",
    "            return 4 * self.bars_count + 1 + 1,\n",
    "        else:\n",
    "            return 3*self.bars_count + 1 + 1,\n",
    "\n",
    "    def encode(self):\n",
    "        \"\"\"\n",
    "        Convert current state into numpy array.\n",
    "        \"\"\"\n",
    "        res = np.ndarray(shape=self.shape, dtype=np.float32)\n",
    "        shift = 0\n",
    "        for bar_idx in range(-self.bars_count+1, 1):\n",
    "            ofs = self._offset + bar_idx\n",
    "            \n",
    "            res[shift] = self._prices.high[ofs]\n",
    "            shift += 1\n",
    "            res[shift] = self._prices.low[ofs]\n",
    "            shift += 1\n",
    "            res[shift] = self._prices.close[ofs]\n",
    "            shift += 1\n",
    "            if self.volumes:\n",
    "                res[shift] = self._prices.volume[ofs]\n",
    "                shift += 1\n",
    "            # print(f\"\"\"state off set ofs {ofs}\\n and shape res as batch from offset {res.shape} \\n \n",
    "            #         state_offset {self._offset} \\n bar_idx {bar_idx} \\n shift {shift} \\n res shift {res}\"\"\")\n",
    "        res[shift] = float(self.have_position)\n",
    "        shift += 1\n",
    "        if not self.have_position:\n",
    "            res[shift] = 0.0\n",
    "        else:\n",
    "            res[shift] = self._cur_close() / self.open_price - 1.0\n",
    "        # print(f\"Final res shape {res.shape} shift {shift}\")\n",
    "        return res\n",
    "\n",
    "    def _cur_close(self):\n",
    "        \"\"\"\n",
    "        Calculate real close price for the current bar\n",
    "        Real Close Price = Open Price * (1.0 + Relative Close Price)\n",
    "                 = 100 * (1.0 + 0.05)\n",
    "                 = 100 * 1.05\n",
    "                 = 105\n",
    "\n",
    "        \"\"\"\n",
    "        open = self._prices.open[self._offset]\n",
    "        rel_close = self._prices.close[self._offset]\n",
    "        return open * (1.0 + rel_close)\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Perform one step in our price, adjust offset, check for the end of prices\n",
    "        and handle position change\n",
    "        :param action:\n",
    "        :return: reward, done\n",
    "        \"\"\"\n",
    "        assert isinstance(action, Actions)\n",
    "        reward = 0.0\n",
    "        done = False\n",
    "        close = self._cur_close()\n",
    "        if action == Actions.Buy and not self.have_position:\n",
    "            self.have_position = True\n",
    "            self.open_price = close\n",
    "            reward -= self.commission_perc\n",
    "        elif action == Actions.Close and self.have_position:\n",
    "            reward -= self.commission_perc\n",
    "            done |= self.reset_on_close\n",
    "            if self.reward_on_close:\n",
    "                reward += 100.0 * (close / self.open_price - 1.0)\n",
    "            self.have_position = False\n",
    "            self.open_price = 0.0\n",
    "\n",
    "        self._offset += 1\n",
    "        prev_close = close\n",
    "        close = self._cur_close()\n",
    "        done |= self._offset >= self._prices.close.shape[0]-1\n",
    "\n",
    "        if self.have_position and not self.reward_on_close:\n",
    "            reward += 100.0 * (close / prev_close - 1.0)\n",
    "\n",
    "        return reward, done\n",
    "\n",
    "\n",
    "class StocksEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    #spec = EnvSpec(\"StocksEnv-v0\",entry_point=libs.envoiran.StocksEnv)\n",
    "\n",
    "    def __init__(self, prices: Prices, bars_count=DEFAULT_BARS_COUNT,\n",
    "                 commission=DEFAULT_COMMISSION_PERC,\n",
    "                 reset_on_close=True, state_1d=False,\n",
    "                 random_ofs_on_reset=True, reward_on_close=False,\n",
    "                 volumes=False):\n",
    "        self._prices = prices\n",
    "        self._state = State(\n",
    "            bars_count, commission, reset_on_close,\n",
    "            reward_on_close=reward_on_close, volumes=volumes)\n",
    "        self.action_space = gym.spaces.Discrete(n=len(Actions))\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=-np.inf, high=np.inf,\n",
    "            shape=self._state.shape, dtype=np.float32)\n",
    "        self.random_ofs_on_reset = random_ofs_on_reset\n",
    "        \n",
    "        #self.seed()\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed1 = seeding.np_random(seed)\n",
    "        seed2 = seeding.hash_seed(seed1 + 1) % 2 ** 31\n",
    "        return [seed1, seed2]\n",
    "    \n",
    "    def reset(self):\n",
    "        self._instrument = self.np_random.choice(\n",
    "            list(self._prices._fields))\n",
    "        if self._instrument is \"open\":\n",
    "            prices = self._prices.open\n",
    "        if self._instrument is \"close\":\n",
    "            prices = self._prices.close\n",
    "        if self._instrument is \"high\":\n",
    "            prices = self._prices.high\n",
    "        if self._instrument is \"low\":\n",
    "            prices = self._prices.low\n",
    "        else:\n",
    "            prices = self._prices.volume\n",
    "        bars = self._state.bars_count\n",
    "        if self.random_ofs_on_reset:\n",
    "            offset = self.np_random.choice(\n",
    "                prices.shape[0]-bars*10) + bars\n",
    "        else:\n",
    "            offset = bars\n",
    "        # print(self._prices.low[offset],offset)\n",
    "        \n",
    "        # return P, offset\n",
    "        self._state.reset(self._prices, offset)\n",
    "        return self._state.encode()\n",
    "    def step(self, action_idx):\n",
    "        \n",
    "        action = Actions(action_idx)\n",
    "        reward, done = self._state.step(action)\n",
    "        obs = self._state.encode()\n",
    "        info = {\n",
    "\n",
    "            \n",
    "                \"instrument\": self._instrument,\n",
    "                \"offset\": self._state._offset\n",
    "                }\n",
    "        return obs, reward, done,info #observation, reward, terminated, truncated, info\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp=preprocess(train_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env  = StocksEnv(rp, bars_count=10,\n",
    "                 commission=0.1,\n",
    "                 reset_on_close=True, state_1d=False,\n",
    "                 random_ofs_on_reset=True, reward_on_close=False,\n",
    "                 volumes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All done for env .\n",
    "lets setup model to get q value from our observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import ptan\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.01\n",
    "EPISODES_TO_TRAIN = 4\n",
    "\n",
    "\n",
    "class PGN(nn.Module):\n",
    "    def __init__(self, input_size, n_actions):\n",
    "        super(PGN, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_qvals(rewards):\n",
    "    res = []\n",
    "    sum_r = 0.0\n",
    "    for r in reversed(rewards):\n",
    "        sum_r *= GAMMA\n",
    "        sum_r += r\n",
    "        res.append(sum_r)\n",
    "    return list(reversed(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PGN(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=42, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "train_path = \"D:\\Practical_Reinforcement_Learning\\chapter8\\data\\ch08-small-quotes\\YNDX_150101_151231.csv\"\n",
    "val_path = \"D:\\Practical_Reinforcement_Learning\\chapter8\\data\\ch08-small-quotes\\YNDX_150101_151231.csv\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "BARS_COUNT = 10\n",
    "\n",
    "EPS_START = 1.0\n",
    "EPS_FINAL = 0.1\n",
    "EPS_STEPS = 1000000\n",
    "\n",
    "GAMMA = 0.99\n",
    "\n",
    "REPLAY_SIZE = 100000\n",
    "REPLAY_INITIAL = 10000\n",
    "REWARD_STEPS = 2\n",
    "LEARNING_RATE = 0.0001\n",
    "STATES_TO_EVALUATE = 1000\n",
    "\n",
    "\n",
    "#env = gym.wrappers.TimeLimit(env, max_episode_steps=1000)\n",
    "# vp = preprocess(val_path)\n",
    "# env_val = StocksEnv(vp, bars_count=10,\n",
    "#                  commission=0.1,\n",
    "#                  reset_on_close=True, state_1d=False,\n",
    "#                  random_ofs_on_reset=True, reward_on_close=False,\n",
    "#                  volumes=True)\n",
    "writer = SummaryWriter(comment=\"-tradingAgent-reinforce\")\n",
    "\n",
    "net = PGN(env.observation_space.shape[0], env.action_space.n)\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "def _group_list(items, lens):\n",
    "    \"\"\"\n",
    "    Unflat the list of items by lens\n",
    "    :param items: list of items\n",
    "    :param lens: list of integers\n",
    "    :return: list of list of items grouped by lengths\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    cur_ofs = 0\n",
    "    for g_len in lens:\n",
    "        res.append(items[cur_ofs:cur_ofs+g_len])\n",
    "        cur_ofs += g_len\n",
    "    return res\n",
    "\n",
    "# one single experience step\n",
    "Experience = namedtuple('Experience', ['state', 'action', 'reward', 'done'])\n",
    "\n",
    "\n",
    "class ProbabilityActionSelector(ptan.actions.ActionSelector):\n",
    "    \"\"\"\n",
    "    Converts probabilities of actions into action by sampling them\n",
    "    \"\"\"\n",
    "    def __call__(self, scores):\n",
    "        assert isinstance(scores, np.ndarray)\n",
    "        probs = np.exp(scores - np.max(scores)) / np.sum(np.exp(scores - np.max(scores)))\n",
    "        actions = []\n",
    "        print(probs)\n",
    "        \n",
    "        for prob in probs:\n",
    "            if np.any(np.isnan(prob)):\n",
    "                action = np.random.choice(len(prob), p=np.array([0.83, 0.02,0.15]))\n",
    "                actions.append(action)\n",
    "            else:\n",
    "                action = np.random.choice(len(prob), p=np.array(prob).reshape(-1))\n",
    "                actions.append(action)\n",
    "\n",
    "        return np.array(actions)\n",
    "    \n",
    "\n",
    "class PolicyAgent(ptan.agent.BaseAgent):\n",
    "    \"\"\"\n",
    "    Policy agent gets action probabilities from the model and samples actions from it\n",
    "    \"\"\"\n",
    "    # TODO: unify code with DQNAgent, as only action selector is differs.\n",
    "    def __init__(self, model, action_selector=ProbabilityActionSelector(), device=\"cpu\",\n",
    "                 apply_softmax=False, preprocessor=ptan.agent.default_states_preprocessor):\n",
    "        self.model = model\n",
    "        self.action_selector = action_selector\n",
    "        self.device = device\n",
    "        self.apply_softmax = apply_softmax\n",
    "        self.preprocessor = preprocessor\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, states, agent_states=None):\n",
    "        \"\"\"\n",
    "        Return actions from given list of states\n",
    "        :param states: list of states\n",
    "        :return: list of actions\n",
    "        \"\"\"\n",
    "        if agent_states is None:\n",
    "            agent_states = [None] * len(states)\n",
    "        if self.preprocessor is not None:\n",
    "            states = self.preprocessor(states)\n",
    "            if torch.is_tensor(states):\n",
    "                states = states.to(self.device)\n",
    "        probs_v = self.model(states)\n",
    "        print(probs_v.size())\n",
    "        if self.apply_softmax:\n",
    "            probs_v = F.log_softmax(probs_v, dim=0)\n",
    "            \n",
    "        probs = probs_v.data.cpu().numpy()\n",
    "        \n",
    "        actions = self.action_selector(probs)\n",
    "        print(actions)\n",
    "        return np.array(actions), agent_states\n",
    "    \n",
    "class ExperienceSource:\n",
    "    \"\"\"\n",
    "    Simple n-step experience source using single or multiple environments\n",
    "\n",
    "    Every experience contains n list of Experience entries\n",
    "    \"\"\"\n",
    "    def __init__(self, env, agent, steps_count=2, steps_delta=1, vectorized=False):\n",
    "        \"\"\"\n",
    "        Create simple experience source\n",
    "        :param env: environment or list of environments to be used\n",
    "        :param agent: callable to convert batch of states into actions to take\n",
    "        :param steps_count: count of steps to track for every experience chain\n",
    "        :param steps_delta: how many steps to do between experience items\n",
    "        :param vectorized: support of vectorized envs from OpenAI universe\n",
    "        \"\"\"\n",
    "        assert isinstance(env, (gym.Env, list, tuple))\n",
    "        assert isinstance(agent, ptan.agent.BaseAgent)\n",
    "        assert isinstance(steps_count, int)\n",
    "        assert steps_count >= 1\n",
    "        assert isinstance(vectorized, bool)\n",
    "        if isinstance(env, (list, tuple)):\n",
    "            self.pool = env\n",
    "        else:\n",
    "            self.pool = [env]\n",
    "        self.agent = agent\n",
    "        self.steps_count = steps_count\n",
    "        self.steps_delta = steps_delta\n",
    "        self.total_rewards = []\n",
    "        self.total_steps = []\n",
    "        self.vectorized = vectorized\n",
    "\n",
    "    def __iter__(self):\n",
    "        states, agent_states, histories, cur_rewards, cur_steps = [], [], [], [], []\n",
    "        env_lens = []\n",
    "        for env in self.pool:\n",
    "            obs = env.reset()\n",
    "            # if the environment is vectorized, all it's output is lists of results.\n",
    "            # Details are here: https://github.com/openai/universe/blob/master/doc/env_semantics.rst\n",
    "            if self.vectorized:\n",
    "                obs_len = len(obs)\n",
    "                states.extend(obs)\n",
    "            else:\n",
    "                obs_len = 1\n",
    "                states.append(obs)\n",
    "            env_lens.append(obs_len)\n",
    "\n",
    "            for _ in range(obs_len):\n",
    "                histories.append(deque(maxlen=self.steps_count))\n",
    "                cur_rewards.append(0.0)\n",
    "                cur_steps.append(0)\n",
    "                agent_states.append(self.agent.initial_state())\n",
    "\n",
    "        iter_idx = 0\n",
    "        while True:\n",
    "            actions = [None] * len(states)\n",
    "            states_input = []\n",
    "            states_indices = []\n",
    "            for idx, state in enumerate(states):\n",
    "                if state is None:\n",
    "                    actions[idx] = self.pool[0].action_space.sample()  # assume that all envs are from the same family\n",
    "                else:\n",
    "                    states_input.append(state)\n",
    "                    states_indices.append(idx)\n",
    "            if states_input:\n",
    "                states_actions, new_agent_states = self.agent(states_input, agent_states)\n",
    "                for idx, action in enumerate(states_actions):\n",
    "                    g_idx = states_indices[idx]\n",
    "                    actions[g_idx] = action\n",
    "                    agent_states[g_idx] = new_agent_states[idx]\n",
    "            grouped_actions = _group_list(actions, env_lens)\n",
    "\n",
    "            global_ofs = 0\n",
    "            for env_idx, (env, action_n) in enumerate(zip(self.pool, grouped_actions)):\n",
    "                if self.vectorized:\n",
    "                    next_state_n, r_n, is_done_n, _ = env.step(action_n)\n",
    "                else:\n",
    "                    next_state, r, is_done, _ = env.step(action_n[0])\n",
    "                    next_state_n, r_n, is_done_n = [next_state], [r], [is_done]\n",
    "\n",
    "                for ofs, (action, next_state, r, is_done) in enumerate(zip(action_n, next_state_n, r_n, is_done_n)):\n",
    "                    idx = global_ofs + ofs\n",
    "                    state = states[idx]\n",
    "                    history = histories[idx]\n",
    "\n",
    "                    cur_rewards[idx] += r\n",
    "                    cur_steps[idx] += 1\n",
    "                    if state is not None:\n",
    "                        history.append(Experience(state=state, action=action, reward=r, done=is_done))\n",
    "                    if len(history) == self.steps_count and iter_idx % self.steps_delta == 0:\n",
    "                        yield tuple(history)\n",
    "                    states[idx] = next_state\n",
    "                    if is_done:\n",
    "                        # in case of very short episode (shorter than our steps count), send gathered history\n",
    "                        if 0 < len(history) < self.steps_count:\n",
    "                            yield tuple(history)\n",
    "                        # generate tail of history\n",
    "                        while len(history) > 1:\n",
    "                            history.popleft()\n",
    "                            yield tuple(history)\n",
    "                        self.total_rewards.append(cur_rewards[idx])\n",
    "                        self.total_steps.append(cur_steps[idx])\n",
    "                        cur_rewards[idx] = 0.0\n",
    "                        cur_steps[idx] = 0\n",
    "                        # vectorized envs are reset automatically\n",
    "                        states[idx] = env.reset() if not self.vectorized else None\n",
    "                        agent_states[idx] = self.agent.initial_state()\n",
    "                        history.clear()\n",
    "                global_ofs += len(action_n)\n",
    "            iter_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "agent = PolicyAgent(net, preprocessor=ptan.agent.float32_preprocessor,\n",
    "                                apply_softmax=True)\n",
    "\n",
    "#exp_source = ExperienceSource(env, agent, gamma=GAMMA,vectorized=)\n",
    "exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=GAMMA,vectorized=False)\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_rewards = []\n",
    "step_idx = 0\n",
    "done_episodes = 0\n",
    "\n",
    "batch_episodes = 0\n",
    "batch_states, batch_actions, batch_qvals = [], [], []\n",
    "cur_rewards = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step_idx, exp in enumerate(exp_source):\n",
    "    batch_states.append(exp.state)\n",
    "    batch_actions.append(int(exp.action))\n",
    "    cur_rewards.append(exp.reward)\n",
    "\n",
    "    if exp.last_state is None:\n",
    "        batch_qvals.extend(calc_qvals(cur_rewards))\n",
    "        cur_rewards.clear()\n",
    "        batch_episodes += 1\n",
    "\n",
    "    # handle new rewards\n",
    "    new_rewards = exp_source.pop_total_rewards()\n",
    "    if new_rewards:\n",
    "        done_episodes += 1\n",
    "        reward = new_rewards[0]\n",
    "        total_rewards.append(reward)\n",
    "        mean_rewards = float(np.mean(total_rewards[-100:]))\n",
    "        print(\"%d: reward: %6.2f, mean_100: %6.2f, episodes: %d\" % (\n",
    "            step_idx, reward, mean_rewards, done_episodes))\n",
    "        writer.add_scalar(\"reward\", reward, step_idx)\n",
    "        writer.add_scalar(\"reward_100\", mean_rewards, step_idx)\n",
    "        writer.add_scalar(\"episodes\", done_episodes, step_idx)\n",
    "        if mean_rewards > 195:\n",
    "            print(\"Solved in %d steps and %d episodes!\" % (step_idx, done_episodes))\n",
    "            break\n",
    "\n",
    "    if batch_episodes < EPISODES_TO_TRAIN:\n",
    "        continue\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    states_v = torch.FloatTensor(batch_states)\n",
    "    batch_actions_t = torch.LongTensor(batch_actions)\n",
    "    batch_qvals_v = torch.FloatTensor(batch_qvals)\n",
    "\n",
    "    logits_v = net(states_v)\n",
    "    log_prob_v = F.log_softmax(logits_v, dim=1)\n",
    "    log_prob_actions_v = batch_qvals_v * log_prob_v[range(len(batch_states)), batch_actions_t]\n",
    "    loss_v = -log_prob_actions_v.mean()\n",
    "\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    batch_episodes = 0\n",
    "    batch_states.clear()\n",
    "    batch_actions.clear()\n",
    "    batch_qvals.clear()\n",
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.model.state_dict(), 'model_state_dict.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VAlidation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = (\n",
    "    'episode_reward',\n",
    "    'episode_steps',\n",
    "    'order_profits',\n",
    "    'order_steps',\n",
    ")\n",
    "\n",
    "def train_batch(engine, batch):\n",
    "    optimizer.zero_grad()\n",
    "    loss_v = calc_loss(batch=batch,net=net,tgt_net=tgt_net,gamma=GAMMA ** REWARD_STEPS, device=device)\n",
    "    #print(loss_v)\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "    eps_tracker.frame(engine.state.iteration)\n",
    "    if getattr(engine.state, \"eval_states\", None) is None:\n",
    "            eval_states = buffer.sample(STATES_TO_EVALUATE)\n",
    "            eval_states = [np.array(transition.state, copy=False)\n",
    "                           for transition in eval_states]\n",
    "            engine.state.eval_states = np.array(eval_states, copy=False)\n",
    "\n",
    "    writer.add_scalar(\"training/loss\", loss_v, engine.state.epoch)\n",
    "    return {\n",
    "        \"loss\": loss_v.item(),\n",
    "        \"epsilon\": selector.epsilon,\n",
    "    }\n",
    "\n",
    "def validation_run(env, net, episodes=100, device=\"cpu\", epsilon=0.02, comission=0.1):\n",
    "    stats = { metric: [] for metric in METRICS }\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        obs = env.reset()\n",
    "\n",
    "        total_reward = 0.0\n",
    "        position = None\n",
    "        position_steps = None\n",
    "        episode_steps = 0\n",
    "\n",
    "        while True:\n",
    "            obs_v = torch.tensor([obs]).to(device)\n",
    "            out_v = net(obs_v)\n",
    "\n",
    "            action_idx = out_v.max(dim=1)[1].item()\n",
    "            if np.random.random() < epsilon:\n",
    "                action_idx = env.action_space.sample()\n",
    "            action = Actions(action_idx)\n",
    "            # closing price at this step\n",
    "            close_price = env._state._cur_close()\n",
    "\n",
    "            if action == Actions.Buy and position is None:\n",
    "                position = close_price\n",
    "                position_steps = 0\n",
    "            elif action == Actions.Close and position is not None:\n",
    "                profit = close_price - position - (close_price + position) * comission / 100\n",
    "                profit = 100.0 * profit / position\n",
    "                stats['order_profits'].append(profit)\n",
    "                stats['order_steps'].append(position_steps)\n",
    "                position = None\n",
    "                position_steps = None\n",
    "\n",
    "            obs, reward, done, _ = env.step(action_idx)\n",
    "            total_reward += reward\n",
    "            episode_steps += 1\n",
    "            if position_steps is not None:\n",
    "                position_steps += 1\n",
    "            if done:\n",
    "                if position is not None:\n",
    "                    profit = close_price - position - (close_price + position) * comission / 100\n",
    "                    profit = 100.0 * profit / position\n",
    "                    stats['order_profits'].append(profit)\n",
    "                    stats['order_steps'].append(position_steps)\n",
    "                break\n",
    "\n",
    "        stats['episode_reward'].append(total_reward)\n",
    "        stats['episode_steps'].append(episode_steps)\n",
    "\n",
    "    return { key: np.mean(vals) for key, vals in stats.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Engine(train_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@trainer.on(Events.COMPLETED | Events.EPOCH_COMPLETED(every=10))\n",
    "def log_training_results(engine):\n",
    "    # Check if current epoch is a multiple of 10\n",
    "    if engine.state.epoch % 10 == 0:\n",
    "        res=validation_run(env_val, net, episodes=100, device=\"cpu\", epsilon=0.02, comission=0.1)\n",
    "        #print(f\"epoch {engine.state.epoch} \\n response : {res}\")\n",
    "        for key, value in res.items():\n",
    "            writer.add_scalar(\"Agent Metrics\",key, value)\n",
    "\n",
    "\n",
    "\n",
    "@trainer.on(Events.ITERATION_COMPLETED)\n",
    "def log_something(engine):\n",
    "    out_dict = engine.state.output\n",
    "    for key, value in out_dict.items():\n",
    "        if value is None:\n",
    "            value = 0.0\n",
    "        elif isinstance(value, torch.Tensor):  # Check if value is a tensor\n",
    "            value = value.item()  # Convert tensor to scalar\n",
    "        writer.add_scalar(f\"Iteration Metrics{engine.state.epoch}/{key}\", value, engine.state.iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sushant\\AppData\\Local\\Temp\\ipykernel_12968\\266377233.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  states_v = torch.tensor(states).to(device)\n",
      "C:\\Users\\sushant\\AppData\\Local\\Temp\\ipykernel_12968\\266377233.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  next_states_v = torch.tensor(next_states).to(device)\n"
     ]
    }
   ],
   "source": [
    "# Checkpointing\n",
    "checkpoint_handler = ModelCheckpoint(dirname='saved_models', filename_prefix='checkpoint', n_saved=2, require_empty=False)\n",
    "trainer.add_event_handler(Events.EPOCH_COMPLETED, checkpoint_handler, {'model': net})\n",
    "trainer.run(batch_generator(buffer, REPLAY_INITIAL, BATCH_SIZE),max_epochs=100)\n",
    "writer.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'model_state_dict.pth')\n",
    "res=validation_run(env_val, net, episodes=100, device=\"cpu\", epsilon=0.02, comission=0.1)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
