{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ptan in c:\\users\\sushant\\anaconda3\\lib\\site-packages (0.4)\n",
      "Collecting opencv-python\n",
      "  Obtaining dependency information for opencv-python from https://files.pythonhosted.org/packages/38/d2/3e8c13ffc37ca5ebc6f382b242b44acb43eb489042e1728407ac3904e72f/opencv_python-4.8.1.78-cp37-abi3-win_amd64.whl.metadata\n",
      "  Downloading opencv_python-4.8.1.78-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\sushant\\anaconda3\\lib\\site-packages (from opencv-python) (1.24.3)\n",
      "Downloading opencv_python-4.8.1.78-cp37-abi3-win_amd64.whl (38.1 MB)\n",
      "   ---------------------------------------- 0.0/38.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/38.1 MB 3.2 MB/s eta 0:00:12\n",
      "   ---------------------------------------- 0.3/38.1 MB 4.3 MB/s eta 0:00:09\n",
      "    --------------------------------------- 0.6/38.1 MB 5.3 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 1.1/38.1 MB 7.0 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 1.6/38.1 MB 8.1 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 3.0/38.1 MB 12.0 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 3.7/38.1 MB 11.9 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 4.1/38.1 MB 11.5 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 5.5/38.1 MB 13.4 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 6.1/38.1 MB 13.8 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 6.5/38.1 MB 13.0 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 7.5/38.1 MB 14.0 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 7.9/38.1 MB 13.6 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 9.2/38.1 MB 14.6 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 9.8/38.1 MB 14.5 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 9.9/38.1 MB 14.0 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 9.9/38.1 MB 14.0 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 9.9/38.1 MB 14.0 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 9.9/38.1 MB 14.0 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 12.3/38.1 MB 14.9 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 12.6/38.1 MB 14.6 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 12.8/38.1 MB 13.6 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 13.9/38.1 MB 13.6 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 15.4/38.1 MB 13.9 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 16.7/38.1 MB 15.6 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 18.3/38.1 MB 17.2 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 19.9/38.1 MB 17.7 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 21.8/38.1 MB 26.2 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 23.1/38.1 MB 31.2 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 23.8/38.1 MB 28.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 24.4/38.1 MB 27.3 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 25.1/38.1 MB 25.2 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 25.9/38.1 MB 25.2 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 26.6/38.1 MB 23.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 27.4/38.1 MB 22.6 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 28.2/38.1 MB 21.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 28.8/38.1 MB 19.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 29.5/38.1 MB 18.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 30.3/38.1 MB 18.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 31.2/38.1 MB 17.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 32.2/38.1 MB 16.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 33.3/38.1 MB 16.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 34.2/38.1 MB 17.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 35.3/38.1 MB 18.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 36.4/38.1 MB 18.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  37.5/38.1 MB 19.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.1/38.1 MB 19.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 38.1/38.1 MB 17.2 MB/s eta 0:00:00\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.8.1.78\n"
     ]
    }
   ],
   "source": [
    "!pip install ptan\n",
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification,AutoFeatureExtractor,AdamW\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import ViTImageProcessor, ViTModel\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ptan\n",
    "import gym\n",
    "import numpy as np\n",
    "from typing import List, Any, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Using GPU.\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    \n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "else:\n",
    "    \n",
    "    print(\"CUDA is not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "# inputs = processor(images=image, return_tensors=\"pt\")\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "# Step 2: Modify classification head for the new number of classes\n",
    "num_classes = 10  # Update with your new number of classes\n",
    "model.classifier = torch.nn.Linear(model.config.hidden_size, num_classes)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = ptan.actions.ArgmaxActionSelector()\n",
    "agent = ptan.agent.PolicyAgent(model=model, action_selector=selector, apply_softmax=True,device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_label = {\n",
    "    0: 'Airplane',\n",
    "    1: 'Automobile',\n",
    "    2: 'Bird',\n",
    "    3: 'Cat',\n",
    "    4: 'Deer',\n",
    "    5: 'Dog',\n",
    "    6: 'Frog',\n",
    "    7: 'Horse',\n",
    "    8: 'Ship',\n",
    "    9: 'Truck'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym.spaces\n",
    "from gym.utils import seeding\n",
    "from gym.envs.registration import EnvSpec\n",
    "import enum\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actions(enum.Enum):\n",
    "    Airplane = 0\n",
    "    Automobile = 1\n",
    "    Bird = 2\n",
    "    Cat = 3\n",
    "    Deer = 4\n",
    "    Dog = 5\n",
    "    Frog = 6\n",
    "    Horse = 7\n",
    "    Ship = 8\n",
    "    Truck = 9\n",
    "\n",
    "# action_string = 'Deer'\n",
    "# if hasattr(Actions, action_string):\n",
    "#     action_enum_member = getattr(Actions, action_string)\n",
    "#     print(action_enum_member)\n",
    "#     print(isinstance(action_enum_member, Actions))  # This will print True\n",
    "# else:\n",
    "#     print(f\"{action_string} is not a valid member of Actions enum.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class state:\n",
    "   def __init__(self,commission_prec,reward_perc):\n",
    "      self.commission_prec=commission_prec\n",
    "      self.reward_perc = reward_perc\n",
    "   def step(self,action,label):\n",
    "      reward = 0.0\n",
    "      done = False\n",
    "      if label is not None:\n",
    "         if hasattr(Actions, label):\n",
    "            label_enum_member = getattr(Actions, label)\n",
    "            if label_enum_member is action:\n",
    "               reward += 100.0 * (1.0 / self.commission_prec - 1.0)\n",
    "               done = True\n",
    "            else:\n",
    "               reward -= self.reward_perc\n",
    "               done = False\n",
    "         return self.observation,reward, done\n",
    "      else:\n",
    "          done = True\n",
    "          return self.observation,reward, done\n",
    "   def update(self, new_observation):\n",
    "        # Update the state with the new observation\n",
    "        self.observation = new_observation\n",
    "   def reset(self,observation):\n",
    "       self.observation = observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10Env(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    spec = EnvSpec(\"CIFAR10Env-v0\",entry_point='CIFAR10Env')\n",
    "    def __init__(self,net ):\n",
    "        super(CIFAR10Env, self).__init__()\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0, high=255,shape=(224, 224, 3), dtype=np.uint8)\n",
    "        self.net = net\n",
    "        self.action_space = gym.spaces.Discrete(n=len(Actions))\n",
    "        self.state = state(0.7,0.3)\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the environment\n",
    "        # Return initial observation\n",
    "        \n",
    "        return np.zeros((10,), dtype=np.float32)\n",
    "    \n",
    "\n",
    "    def update(self, new_observation_path):\n",
    "        # Update the state with the new observation\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "        image_path = new_observation_path\n",
    "        image = Image.open(image_path)\n",
    "        image_tensor = transform(image)\n",
    "        # image_tensor = (image_tensor - image_tensor.min()) / (image_tensor.max() - image_tensor.min())  # Normalize to [0, 1]\n",
    "\n",
    "        # Step 3: Forward pass\n",
    "        inputs = image_tensor.unsqueeze(0).to(device)  # Add batch dimension\n",
    "        self.observation_space = inputs\n",
    "        self.state.update(self.observation_space)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, label):\n",
    "        # Take a step in the environment based on the given action\n",
    "        # Return new observation, reward, done, and info\n",
    "\n",
    "        outputs = self.net(self.observation_space)\n",
    "\n",
    "        # Step 4: Post-process predictions\n",
    "        # Assuming 'logits' is the key returned by your model\n",
    "        logits = outputs.logits if hasattr(outputs, \"logits\") else outputs.last_hidden_state\n",
    "        probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
    "        predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "        predicted_label = index_to_label[predicted_class]\n",
    "        print(predicted_label)\n",
    "        action = getattr(Actions, predicted_label)\n",
    "\n",
    "        obs,reward, done = self.state.step(action,label)\n",
    "        info = {\n",
    "            \"obs\": obs,\n",
    "            \"reward\": reward,\n",
    "            \"done\":done\n",
    "        }\n",
    "        return obs,reward, done, info,logits\n",
    "    def set_observation(self,input_frame):\n",
    "        self.observation_space = input_frame\n",
    "\n",
    "    def _reset_environment(self):\n",
    "        # Implement your environment's reset logic here and return the initial observation\n",
    "        # This is a placeholder, replace it with your own environment's reset logic\n",
    "        initial_observation = np.random.randint(0, 256, size=(224, 224, 3), dtype=np.uint8)\n",
    "        return initial_observation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(10)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = CIFAR10Env(model)\n",
    "env.action_space\n",
    "env.update('0002.jpg')\n",
    "print(env.observation_space)\n",
    "o,r,d,_,loggits=env.step(\"Bird\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0592,  0.0155, -0.0229, -0.0412, -0.1823, -0.0933,  0.0322,  0.1976,\n",
      "         -0.0753,  0.2858]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(loggits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
