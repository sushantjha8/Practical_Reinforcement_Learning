{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pygame\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import ptan\n",
    "import pygame\n",
    "\n",
    "class CartPoleEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(CartPoleEnv, self).__init__()\n",
    "        self.env = gym.make(\"CartPole-v1\")\n",
    "        self.action_space = self.env.action_space\n",
    "        self.observation_space = self.env.observation_space\n",
    "\n",
    "    def reset(self):\n",
    "        return self.env.reset()\n",
    "\n",
    "    def step(self, action):\n",
    "        next_obs, reward, _is_done, _, _ = self.env.step(action)\n",
    "        return next_obs, reward, _is_done, {}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Reinforcement Learning with PyTorch and PTAN: A Practical Guide\n",
    "\n",
    "Title: Deep Reinforcement Learning with PyTorch and PTAN: A Practical Guide\n",
    "Introduction\n",
    "Reinforcement Learning (RL) is a powerful paradigm in machine learning, and its application to deep neural networks has led to significant advancements. In this blog post, we'll explore a practical implementation of a Deep Q-Network (DQN) using PyTorch and PTAN (PyTorch Agent Net) and discuss the benefits of deploying this architecture.\n",
    "\n",
    "Environment Setup\n",
    "To get started, we'll create a custom RL environment using the CIFAR-10 dataset. The environment provides a simplified interface for training an agent to perform tasks related to image classification. It utilizes PyTorch's DataLoader for efficient data handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class DQN(nn.Module):\n",
    "#     def __init__(self, input_size, n_actions):\n",
    "#         super(DQN, self).__init__()\n",
    "\n",
    "#         self.fc_layers = nn.Sequential(\n",
    "#             nn.Linear(input_size, 128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(128, n_actions)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         print(\"model forward\")\n",
    "#         x = self.fc_layers(x)\n",
    "\n",
    "#         return x\n",
    "# class LinearAlgebraActionSelector(ptan.actions.ActionSelector):\n",
    "#     def __call__(self, q_values):\n",
    "#         # Round the output to obtain integer predictions\n",
    "#         #q_values = torch.round(q_values)\n",
    "#         print(f\"Running Action selector {np.argmax(q_values)}\")\n",
    "#         #action = np.argmax(q_values)  # Corrected method name\n",
    "#         return q_values\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQN Architecture\n",
    "The DQN is a convolutional neural network (CNN) designed to process image observations. The architecture includes convolutional layers followed by fully connected layers. PyTorch's neural network module is employed for constructing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n",
      "Action: 1, Reward: 1.0, Done: False Obs: [ 0.0504558   0.23710015 -0.01167121 -0.2672277 ]\n",
      "Action: 1, Reward: 1.0, Done: False Obs: [ 0.0551978   0.43238673 -0.01701577 -0.56356883]\n",
      "Action: 1, Reward: 1.0, Done: False Obs: [ 0.06384553  0.62774324 -0.02828714 -0.8615636 ]\n",
      "Action: 1, Reward: 1.0, Done: False Obs: [ 0.0764004   0.82323873 -0.04551842 -1.1630049 ]\n",
      "Action: 0, Reward: 1.0, Done: False Obs: [ 0.09286518  0.6287381  -0.06877851 -0.8849339 ]\n",
      "Action: 0, Reward: 1.0, Done: False Obs: [ 0.10543993  0.434614   -0.08647719 -0.61464113]\n",
      "Action: 1, Reward: 1.0, Done: False Obs: [ 0.11413222  0.63083106 -0.09877001 -0.9332595 ]\n",
      "Action: 1, Reward: 1.0, Done: False Obs: [ 0.12674883  0.8271369  -0.1174352  -1.2552743 ]\n",
      "Action: 1, Reward: 1.0, Done: False Obs: [ 0.14329158  1.0235502  -0.1425407  -1.5823116 ]\n",
      "Action: 1, Reward: 1.0, Done: False Obs: [ 0.16376257  1.2200516  -0.17418692 -1.9158397 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Practical_Reinforcement_Learning\\.conda\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "# Example of using the CartPoleEnv\n",
    "env = CartPoleEnv()\n",
    "\n",
    "# Reset the environment\n",
    "obs = env.reset()\n",
    "print(env.action_space)\n",
    "# Sample random actions for 10 steps\n",
    "for _ in range(10):\n",
    "    action = env.action_space.sample()\n",
    "    next_obs, reward, _is_done, _= env.step(action)\n",
    "    \n",
    "    print(f\"Action: {action}, Reward: {reward}, Done: {_is_done} Obs: {next_obs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PTAN: PyTorch Agent Net\n",
    "PTAN is a high-level library that simplifies the implementation of reinforcement learning algorithms. It provides useful abstractions for experience replay buffers, action selectors, and agent interfaces. The DQNAgent class from PTAN is utilized to connect the DQN model with the RL environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple Experience class for illustration purposes\n",
    "class Experience:\n",
    "    def __init__(self, state, action, reward, done, next_state):\n",
    "        self.state = state\n",
    "        self.action = action\n",
    "        self.reward = reward\n",
    "        self.done = done\n",
    "        self.next_state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Practical_Reinforcement_Learning\\.conda\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get the size of the observation space for CartPole\n",
    "obs_size = env.observation_space.shape[0]\n",
    "\n",
    "# Create CartPole environment\n",
    "env = CartPoleEnv()\n",
    "\n",
    "# Neural network and optimizer\n",
    "net = DQN(obs_size, env.action_space.n)\n",
    "target_net = ptan.agent.TargetNet(net)\n",
    "selector = ptan.actions.ArgmaxActionSelector()\n",
    "epsilon_greedy = ptan.actions.EpsilonGreedyActionSelector(epsilon=0.1, selector=selector)\n",
    "# action_selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=0.1)\n",
    "agent = ptan.agent.DQNAgent(net, action_selector=epsilon_greedy, preprocessor=ptan.agent.float32_preprocessor)\n",
    "\n",
    "# Experience source\n",
    "exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=0.99, steps_count=1)\n",
    "\n",
    "# Experience buffer\n",
    "buffer = ptan.experience.ExperienceReplayBuffer(exp_source, buffer_size=1000)\n",
    "\n",
    "# Loss function and optimizer\n",
    "loss_func = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
    "\n",
    "best_reward = float('-inf')  # Initialize with negative infinity or other appropriate value\n",
    "best_model_path = \"best_model.pth\"  # Define the path where you want to save the best model\n",
    "checkpoint_interval = 5\n",
    "current_reward = 0.0\n",
    "prev_loss = float('-inf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "# env.reset()\n",
    "# # Get the size of the observation space for CartPole\n",
    "# obs_size = env.observation_space.shape[0]\n",
    "# net = DQN(obs_size, env.action_space.n)\n",
    "# target_net = ptan.agent.TargetNet(net)\n",
    "# #selector = ptan.actions.ArgmaxActionSelector()\n",
    "# selector = LinearAlgebraActionSelector()\n",
    "# #epsilon_greedy = ptan.actions.EpsilonGreedyActionSelector(epsilon=0.1, selector=selector)\n",
    "# agent = ptan.agent.DQNAgent(net, action_selector=selector, preprocessor=float32_preprocessor)\n",
    "# # Experience source\n",
    "# exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=0.99, steps_count=1)\n",
    "\n",
    "# # Experience buffer\n",
    "# buffer = ptan.experience.ExperienceReplayBuffer(exp_source, buffer_size=1000)\n",
    "# next_obs, reward, _is_done, _, _ = env.step(env.action_space.sample())\n",
    "# while True:\n",
    "#     env.render()\n",
    "    \n",
    "#     # Wrap the state in a batch-like structure\n",
    "    \n",
    "    \n",
    "#     action = agent([next_obs])\n",
    "#     next_obs, reward, done, _, _ = env.step(action[0].item())  # Extract the action value from the tensor\n",
    "#     print(reward,done)\n",
    "#     # exp_source.append(state=state, action=action, reward=reward, last_state=nstate, done=done)\n",
    "#     if done:\n",
    "#         break\n",
    "\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ptan.experience.ExperienceReplayBuffer object at 0x00000273C1597390>\n"
     ]
    }
   ],
   "source": [
    "print(buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n",
      "max_len 4\n",
      "observation state [-0.04818577  0.00036734  0.04510041  0.02190833]\n",
      "pad state uptorch.Size([4])\n",
      "[tensor([-0.0482,  0.0004,  0.0451,  0.0219])]\n",
      " np_states shape (1, 4)\n",
      "model forward\n",
      "max_len 1\n",
      "observation state -0.04817841947078705\n",
      "pad state dow : torch.Size([4])\n",
      "[tensor([-0.0482, -0.0482, -0.0482, -0.0482])]\n",
      " np_states shape (1, 4)\n",
      "model forward\n",
      "max_len 1\n",
      "observation state -0.044282130897045135\n",
      "pad state dow : torch.Size([4])\n",
      "[tensor([-0.0443, -0.0443, -0.0443, -0.0443])]\n",
      " np_states shape (1, 4)\n",
      "model forward\n",
      "max_len 1\n",
      "observation state -0.036496978253126144\n",
      "pad state dow : torch.Size([4])\n",
      "[tensor([-0.0365, -0.0365, -0.0365, -0.0365])]\n",
      " np_states shape (1, 4)\n",
      "model forward\n",
      "max_len 1\n",
      "observation state -0.02482120506465435\n",
      "pad state dow : torch.Size([4])\n",
      "[tensor([-0.0248, -0.0248, -0.0248, -0.0248])]\n",
      " np_states shape (1, 4)\n",
      "model forward\n",
      "max_len 1\n",
      "observation state -0.009251383133232594\n",
      "pad state dow : torch.Size([4])\n",
      "[tensor([-0.0093, -0.0093, -0.0093, -0.0093])]\n",
      " np_states shape (1, 4)\n",
      "model forward\n",
      "max_len 1\n",
      "observation state 0.010217283852398396\n",
      "pad state dow : torch.Size([4])\n",
      "[tensor([0.0102, 0.0102, 0.0102, 0.0102])]\n",
      " np_states shape (1, 4)\n",
      "model forward\n",
      "max_len 1\n",
      "observation state 0.03359048441052437\n",
      "pad state dow : torch.Size([4])\n",
      "[tensor([0.0336, 0.0336, 0.0336, 0.0336])]\n",
      " np_states shape (1, 4)\n",
      "model forward\n",
      "max_len 1\n",
      "observation state 0.060874130576848984\n",
      "pad state dow : torch.Size([4])\n",
      "[tensor([0.0609, 0.0609, 0.0609, 0.0609])]\n",
      " np_states shape (1, 4)\n",
      "model forward\n",
      "max_len 1\n",
      "observation state 0.09207341820001602\n",
      "pad state dow : torch.Size([4])\n",
      "[tensor([0.0921, 0.0921, 0.0921, 0.0921])]\n",
      " np_states shape (1, 4)\n",
      "model forward\n",
      "max_len 1\n",
      "observation state 0.12719161808490753\n",
      "pad state dow : torch.Size([4])\n",
      "[tensor([0.1272, 0.1272, 0.1272, 0.1272])]\n",
      " np_states shape (1, 4)\n",
      "model forward\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Practical_Reinforcement_Learning\\.conda\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "def float32_preprocessor(states):\n",
    "    # Assuming states is a list of arrays with varying lengths\n",
    "    max_len = max(len(state[0]) if hasattr(state[0], '__len__') and len(state[0]) > 0 else 1 for state in states)\n",
    "    print(f\"max_len {max_len}\")\n",
    "    p_states = []\n",
    "\n",
    "    for state in states:\n",
    "        observation_state = state[0]\n",
    "        print(f\"observation state {observation_state}\")\n",
    "        if hasattr(observation_state, '__len__') and len(observation_state) > 0:\n",
    "            padded_state = torch.nn.functional.pad(torch.tensor(observation_state, dtype=torch.float32), (0, max_len - len(observation_state)))\n",
    "            print(f\"pad state up{padded_state.shape }\")\n",
    "        else:\n",
    "            # Handle scalar values (e.g., NumPy float) by repeating them to the fixed length\n",
    "            padded_state = torch.tensor([observation_state] * max_len, dtype=torch.float32)\n",
    "            padded_state= padded_state.expand(4)\n",
    "            print(f\"pad state dow : {padded_state.shape}\")\n",
    "        p_states.append(padded_state)\n",
    "        print(p_states)\n",
    "    # Stack the padded states along a new dimension\n",
    "        np_states = np.stack(p_states, axis=0)\n",
    "\n",
    "    print(f\" np_states shape {np_states.shape}\")\n",
    "    return torch.tensor(np_states)\n",
    "\n",
    "\n",
    "def is_done(_, last_state):\n",
    "    return last_state is None  # Example condition, adjust as needed\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_size, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"model forward\")\n",
    "        x = self.fc_layers(x)\n",
    "\n",
    "        return x\n",
    "# Create CartPole environment\n",
    "env = CartPoleEnv()\n",
    "\n",
    "# Neural network and optimizer\n",
    "obs_size = env.observation_space.shape[0]\n",
    "print(env.action_space)\n",
    "net = DQN(obs_size, env.action_space.n)\n",
    "target_net = ptan.agent.TargetNet(net)\n",
    "selector = ptan.actions.ArgmaxActionSelector()\n",
    "epsilon_greedy = ptan.actions.EpsilonGreedyActionSelector(epsilon=0.1, selector=selector)\n",
    "agent = ptan.agent.DQNAgent(net, action_selector=epsilon_greedy, preprocessor=float32_preprocessor)\n",
    "\n",
    "# Experience source\n",
    "exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=0.99, steps_count=10)#,vectorized=True, is_done=is_done)\n",
    "\n",
    "# Experience buffer\n",
    "buffer = ptan.experience.ExperienceReplayBuffer(exp_source, buffer_size=1000)\n",
    "\n",
    "# Populate the buffer with 100 experiences\n",
    "buffer.populate(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__add__', '__class__', '__class_getitem__', '__contains__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getnewargs__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__match_args__', '__module__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__rmul__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '_asdict', '_field_defaults', '_fields', '_make', '_replace', 'action', 'count', 'index', 'last_state', 'reward', 'state']\n",
      "[-0.03356702 -0.4147404   0.01723967  0.6291015 ]\n",
      "0\n",
      "6.793465209301\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#buffer.populate(1)\n",
    "\n",
    "    # Get batch from the buffer\n",
    "batch = buffer.sample(1)\n",
    "for experience in batch:\n",
    "        print(dir(experience))\n",
    "        print(experience.state)\n",
    "        print(experience.action)\n",
    "        print(experience.reward)\n",
    "        print(experience.last_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Practical_Reinforcement_Learning\\.conda\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Loss function and optimizer\n",
    "loss_func = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
    "\n",
    "best_reward = float('-inf')  # Initialize with negative infinity or other appropriate value\n",
    "best_model_path = \"best_model.pth\"  # Define the path where you want to save the best model\n",
    "checkpoint_interval = 5\n",
    "current_reward = 0.0\n",
    "prev_loss = float('-inf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Loop\n",
    "The training loop involves interacting with the environment, collecting experiences, and updating the DQN model. We utilize an experience replay buffer to store and sample experiences, enabling more stable and efficient learning.\n",
    "\n",
    "\n",
    "Saving the Best Model\n",
    "To ensure that the best-performing model is saved during training, we track the highest achieved reward and save the model's state dictionary accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_batch(batch):\n",
    "    states, actions, rewards, next_states = [], [], [], []\n",
    "\n",
    "    for experience in batch:\n",
    "        print(experience.state)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        # if experience.last_state == None:\n",
    "        #     print(\"One or more lists are empty, skipping...\")\n",
    "        #     continue  # You might want to return something meaningful or raise an exception here\n",
    "\n",
    "        # Check if the lengths of states, actions, rewards, and next_states are consistent\n",
    "        if len(states) != len(actions) or len(states) != len(rewards) or len(states):# != len(next_states):\n",
    "            print(\"Inconsistent lengths of states, actions, rewards, or next_states, skipping...\")\n",
    "            continue  # You might want to return something meaningful or raise an exception here\n",
    "        states.extend(experience.state)  # Use extend instead of append\n",
    "        actions.append(experience.action)\n",
    "        rewards.extend([experience.reward])  # Use extend instead of append\n",
    "        #next_states.extend(experience.last_state)  # Use extend instead of append\n",
    "\n",
    "    # Convert lists to PyTorch tensors\n",
    "    states_v = torch.tensor(states, dtype=torch.float32)\n",
    "    actions_v = torch.tensor(actions)\n",
    "    rewards_v = torch.tensor(rewards, dtype=torch.float32)\n",
    "    next_states_v = torch.tensor(next_states, dtype=torch.float32)\n",
    "\n",
    "    return states_v, actions_v, rewards_v, states_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Practical_Reinforcement_Learning\\chapter6\\carpolebasicdqn.ipynb Cell 19\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/carpolebasicdqn.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Call the unpack_batch function\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/carpolebasicdqn.ipynb#X24sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m batch_result \u001b[39m=\u001b[39m unpack_batch(batch)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/carpolebasicdqn.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Check if the result is not None before using the unpacked values\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/carpolebasicdqn.ipynb#X24sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mif\u001b[39;00m batch_result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'batch' is not defined"
     ]
    }
   ],
   "source": [
    "# Call the unpack_batch function\n",
    "batch_result = unpack_batch(batch)\n",
    "\n",
    "# Check if the result is not None before using the unpacked values\n",
    "if batch_result is not None:\n",
    "    states_v, actions_v, rewards_v, next_states_v = batch_result\n",
    "    # Use states_v, actions_v, rewards_v, next_states_v in your further processing or training\n",
    "else:\n",
    "    print(\"Skipping batch due to unpack_batch returning None.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.04428213  0.38925767  0.04041436 -0.53418946]\n",
      "train loop statetensor([-0.0443,  0.3893,  0.0404, -0.5342]),acttensor([1]), rewardtensor([8.6483]), nstensor([-0.0443,  0.3893,  0.0404, -0.5342])\n",
      "train loop statetorch.Size([4]),acttensor([1]), rewardtensor([8.6483]), nstensor([-0.0443,  0.3893,  0.0404, -0.5342])\n",
      "model forward\n",
      "[-0.03649698  0.58378863  0.02973057 -0.8138691 ]\n",
      "train loop statetensor([-0.0365,  0.5838,  0.0297, -0.8139]),acttensor([1]), rewardtensor([7.7255]), nstensor([-0.0365,  0.5838,  0.0297, -0.8139])\n",
      "train loop statetorch.Size([4]),acttensor([1]), rewardtensor([7.7255]), nstensor([-0.0365,  0.5838,  0.0297, -0.8139])\n",
      "model forward\n",
      "[-0.04817842  0.19481446  0.04553857 -0.25621074]\n",
      "train loop statetensor([-0.0482,  0.1948,  0.0455, -0.2562]),acttensor([1]), rewardtensor([9.5618]), nstensor([-0.0482,  0.1948,  0.0455, -0.2562])\n",
      "train loop statetorch.Size([4]),acttensor([1]), rewardtensor([9.5618]), nstensor([-0.0482,  0.1948,  0.0455, -0.2562])\n",
      "model forward\n",
      "[ 0.01021728  1.16866    -0.03619762 -1.6808109 ]\n",
      "train loop statetensor([ 0.0102,  1.1687, -0.0362, -1.6808]),acttensor([1]), rewardtensor([4.9010]), nstensor([ 0.0102,  1.1687, -0.0362, -1.6808])\n",
      "train loop statetorch.Size([4]),acttensor([1]), rewardtensor([4.9010]), nstensor([ 0.0102,  1.1687, -0.0362, -1.6808])\n",
      "model forward\n",
      "[ 0.01021728  1.16866    -0.03619762 -1.6808109 ]\n",
      "train loop statetensor([ 0.0102,  1.1687, -0.0362, -1.6808]),acttensor([1]), rewardtensor([4.9010]), nstensor([ 0.0102,  1.1687, -0.0362, -1.6808])\n",
      "train loop statetorch.Size([4]),acttensor([1]), rewardtensor([4.9010]), nstensor([ 0.0102,  1.1687, -0.0362, -1.6808])\n",
      "model forward\n",
      "[ 0.03359048  1.3641822  -0.06981383 -1.9845419 ]\n",
      "train loop statetensor([ 0.0336,  1.3642, -0.0698, -1.9845]),acttensor([1]), rewardtensor([3.9404]), nstensor([ 0.0336,  1.3642, -0.0698, -1.9845])\n",
      "train loop statetorch.Size([4]),acttensor([1]), rewardtensor([3.9404]), nstensor([ 0.0336,  1.3642, -0.0698, -1.9845])\n",
      "model forward\n",
      "[ 0.09207342  1.75591    -0.15546484 -2.6222963 ]\n",
      "train loop statetensor([ 0.0921,  1.7559, -0.1555, -2.6223]),acttensor([1]), rewardtensor([1.9900]), nstensor([ 0.0921,  1.7559, -0.1555, -2.6223])\n",
      "train loop statetorch.Size([4]),acttensor([1]), rewardtensor([1.9900]), nstensor([ 0.0921,  1.7559, -0.1555, -2.6223])\n",
      "model forward\n",
      "[ 0.03359048  1.3641822  -0.06981383 -1.9845419 ]\n",
      "train loop statetensor([ 0.0336,  1.3642, -0.0698, -1.9845]),acttensor([1]), rewardtensor([3.9404]), nstensor([ 0.0336,  1.3642, -0.0698, -1.9845])\n",
      "train loop statetorch.Size([4]),acttensor([1]), rewardtensor([3.9404]), nstensor([ 0.0336,  1.3642, -0.0698, -1.9845])\n",
      "model forward\n",
      "max_len 4\n",
      "observation state [-0.0045294  -0.04443692 -0.03541702  0.03674437]\n",
      "pad state uptorch.Size([4])\n",
      "[tensor([-0.0045, -0.0444, -0.0354,  0.0367])]\n",
      " np_states shape (1, 4)\n",
      "model forward\n",
      "max_len 1\n",
      "observation state -0.005418133456259966\n",
      "pad state dow : torch.Size([4])\n",
      "[tensor([-0.0054, -0.0054, -0.0054, -0.0054])]\n",
      " np_states shape (1, 4)\n",
      "model forward\n",
      "max_len 1\n",
      "observation state -0.010198804549872875\n",
      "pad state dow : torch.Size([4])\n",
      "[tensor([-0.0102, -0.0102, -0.0102, -0.0102])]\n",
      " np_states shape (1, 4)\n",
      "model forward\n",
      "max_len 1\n",
      "observation state -0.011067509651184082\n",
      "pad state dow : torch.Size([4])\n",
      "[tensor([-0.0111, -0.0111, -0.0111, -0.0111])]\n",
      " np_states shape (1, 4)\n",
      "model forward\n",
      "max_len 1\n",
      "observation state -0.008025885559618473\n",
      "pad state dow : torch.Size([4])\n",
      "[tensor([-0.0080, -0.0080, -0.0080, -0.0080])]\n",
      " np_states shape (1, 4)\n",
      "model forward\n",
      "max_len 1\n",
      "observation state -0.0010740576544776559\n",
      "pad state dow : torch.Size([4])\n",
      "[tensor([-0.0011, -0.0011, -0.0011, -0.0011])]\n",
      " np_states shape (1, 4)\n",
      "model forward\n",
      "max_len 1\n",
      "observation state 0.009789332747459412\n",
      "pad state dow : torch.Size([4])\n",
      "[tensor([0.0098, 0.0098, 0.0098, 0.0098])]\n",
      " np_states shape (1, 4)\n",
      "model forward\n",
      "max_len 1\n",
      "observation state 0.024566905573010445\n",
      "pad state dow : torch.Size([4])\n",
      "[tensor([0.0246, 0.0246, 0.0246, 0.0246])]\n",
      " np_states shape (1, 4)\n",
      "model forward\n",
      "max_len 1\n",
      "observation state 0.043262120336294174\n",
      "pad state dow : torch.Size([4])\n",
      "[tensor([0.0433, 0.0433, 0.0433, 0.0433])]\n",
      " np_states shape (1, 4)\n",
      "model forward\n",
      "max_len 1\n",
      "observation state 0.06587862968444824\n",
      "pad state dow : torch.Size([4])\n",
      "[tensor([0.0659, 0.0659, 0.0659, 0.0659])]\n",
      " np_states shape (1, 4)\n",
      "model forward\n",
      "max_len 1\n",
      "observation state 0.09241937845945358\n",
      "pad state dow : torch.Size([4])\n",
      "[tensor([0.0924, 0.0924, 0.0924, 0.0924])]\n",
      " np_states shape (1, 4)\n",
      "model forward\n",
      "[-0.02482121  0.7784911   0.01345319 -1.0970542 ]\n",
      "train loop statetensor([-0.0248,  0.7785,  0.0135, -1.0971]),acttensor([1]), rewardtensor([6.7935]), nstensor([-0.0248,  0.7785,  0.0135, -1.0971])\n",
      "train loop statetorch.Size([4]),acttensor([1]), rewardtensor([6.7935]), nstensor([-0.0248,  0.7785,  0.0135, -1.0971])\n",
      "model forward\n",
      "[ 0.06087413  1.5599644  -0.10950467 -2.2980084 ]\n",
      "train loop statetensor([ 0.0609,  1.5600, -0.1095, -2.2980]),acttensor([1]), rewardtensor([2.9701]), nstensor([ 0.0609,  1.5600, -0.1095, -2.2980])\n",
      "train loop statetorch.Size([4]),acttensor([1]), rewardtensor([2.9701]), nstensor([ 0.0609,  1.5600, -0.1095, -2.2980])\n",
      "model forward\n",
      "(array([-0.0045294 , -0.04443692, -0.03541702,  0.03674437], dtype=float32), {})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sushant\\AppData\\Local\\Temp\\ipykernel_15560\\1739219189.py:24: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  states_v = torch.tensor(states, dtype=torch.float32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 4 at dim 1 (got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Practical_Reinforcement_Learning\\chapter6\\carpolebasicdqn.ipynb Cell 20\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/carpolebasicdqn.ipynb#X25sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Get batch from the buffer\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/carpolebasicdqn.ipynb#X25sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m batch \u001b[39m=\u001b[39m buffer\u001b[39m.\u001b[39msample(\u001b[39m1\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/carpolebasicdqn.ipynb#X25sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m states_v, actions_v, rewards_v, next_states_v \u001b[39m=\u001b[39m unpack_batch(batch)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/carpolebasicdqn.ipynb#X25sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrain loop state\u001b[39m\u001b[39m{\u001b[39;00mstates_v\u001b[39m}\u001b[39;00m\u001b[39m,act\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m \u001b[39mactions_v\u001b[39m}\u001b[39;00m\u001b[39m, reward\u001b[39m\u001b[39m{\u001b[39;00mrewards_v\u001b[39m}\u001b[39;00m\u001b[39m, ns\u001b[39m\u001b[39m{\u001b[39;00mnext_states_v\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/carpolebasicdqn.ipynb#X25sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrain loop state\u001b[39m\u001b[39m{\u001b[39;00mstates_v\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m,act\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m \u001b[39mactions_v\u001b[39m}\u001b[39;00m\u001b[39m, reward\u001b[39m\u001b[39m{\u001b[39;00mrewards_v\u001b[39m}\u001b[39;00m\u001b[39m, ns\u001b[39m\u001b[39m{\u001b[39;00mnext_states_v\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32md:\\Practical_Reinforcement_Learning\\chapter6\\carpolebasicdqn.ipynb Cell 20\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/carpolebasicdqn.ipynb#X25sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     rewards\u001b[39m.\u001b[39mextend([experience\u001b[39m.\u001b[39mreward])  \u001b[39m# Use extend instead of append\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/carpolebasicdqn.ipynb#X25sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39m#next_states.extend(experience.last_state)  # Use extend instead of append\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/carpolebasicdqn.ipynb#X25sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/carpolebasicdqn.ipynb#X25sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# Convert lists to PyTorch tensors\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/carpolebasicdqn.ipynb#X25sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m states_v \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(states, dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat32)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/carpolebasicdqn.ipynb#X25sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m actions_v \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(actions)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/carpolebasicdqn.ipynb#X25sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m rewards_v \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(rewards, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 4 at dim 1 (got 0)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Training loop\n",
    "for step in range(1000):  # You may want to adjust the number of steps\n",
    "    buffer.populate(1)\n",
    "\n",
    "    # Get batch from the buffer\n",
    "    batch = buffer.sample(1)\n",
    "    states_v, actions_v, rewards_v, next_states_v = unpack_batch(batch)\n",
    "\n",
    "    print(f\"train loop state{states_v},act{ actions_v}, reward{rewards_v}, ns{next_states_v}\")\n",
    "    print(f\"train loop state{states_v.shape},act{ actions_v}, reward{rewards_v}, ns{next_states_v}\")\n",
    "    # Zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    if states_v is not None:\n",
    "        # Forward pass\n",
    "        expected_state_dimension = 4  # Adjust this based on your expected state dimension\n",
    "        if len(states_v) == expected_state_dimension:\n",
    "            q_values = net(states_v)\n",
    "        else:\n",
    "            print(\"skipping\")\n",
    "\n",
    "    # # Get target Q-values\n",
    "    # target_q_values = target_net.target_model(next_states_v).max(1)[0]\n",
    "\n",
    "    # # Use view(-1) to ensure dones_mask is 1-dimensional\n",
    "    # dones_mask = dones_mask.view(-1)\n",
    "\n",
    "    # # Ensure target_q_values is also 1-dimensional\n",
    "    # target_q_values = target_q_values.view(-1)\n",
    "\n",
    "    # # Detach target_q_values\n",
    "    # target_q_values = target_q_values.detach()\n",
    "\n",
    "    # # Calculate TD error\n",
    "    # expected_q_values = rewards_v + target_q_values * 0.99\n",
    "    # loss = loss_func(q_values, expected_q_values)\n",
    "\n",
    "#     # Backward pass\n",
    "#     loss.backward()\n",
    "\n",
    "#     # Optimize\n",
    "#     optimizer.step()\n",
    "\n",
    "#     if step % 10 == 0:\n",
    "#         target_net.sync()\n",
    "    \n",
    "#     if prev_loss > loss.item():\n",
    "#         prev_loss = loss.item()\n",
    "#         print(f\"\\n----------------------------Loss {loss.item()}------------\\n\")\n",
    "#         if step % checkpoint_interval == 0:\n",
    "#             # Check the performance and save the model if it's the best so far\n",
    "#             if current_reward > best_reward:\n",
    "#                 best_reward = current_reward\n",
    "#                 torch.save(net.state_dict(), best_model_path)\n",
    "#                 print(f\"\\n----------------------------checkpoint saved ------------\\n\")\n",
    "\n",
    "# # After training, you can use the trained model for inference.\n",
    "# For example, you can run the trained agent in the environment:\n",
    "# env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benefits of Deployment\n",
    "1. Stability and Efficiency:\n",
    "The use of experience replay buffers enhances stability during training by reducing the impact of correlated experiences.\n",
    "Efficient learning is achieved through the replay buffer, allowing the model to revisit and learn from past experiences.\n",
    "2. Customizable Environments:\n",
    "The modular design of the custom environment allows for easy adaptation to various tasks beyond image classification.\n",
    "Customization provides flexibility for addressing specific problem domains.\n",
    "3. PyTorch and PTAN Integration:\n",
    "Leveraging PyTorch for building the DQN model provides a seamless experience for researchers and practitioners familiar with the PyTorch ecosystem.\n",
    "PTAN simplifies the RL implementation, making it more accessible and reducing the boilerplate code.\n",
    "4. Best Model Selection:\n",
    "The implementation includes a mechanism for tracking the best-performing model during training, ensuring that the model with the highest reward is saved.\n",
    "5. Inference with Trained Model:\n",
    "The trained model can be easily loaded for inference, allowing users to deploy the RL agent in real-world scenarios.\n",
    "Conclusion\n",
    "In this blog post, we've walked through the implementation of a Deep Q-Network using PyTorch and PTAN, highlighting the benefits of this architecture. This example serves as a foundation for understanding and applying deep reinforcement learning techniques to custom environments. The flexibility, stability, and efficiency provided by this deployment make it a valuable tool for a wide range of RL applications.\n",
    "\n",
    "To access the complete code and resources, please refer to the GitHub repository linked [here]."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
