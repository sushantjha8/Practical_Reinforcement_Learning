{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Action: 5, Reward: 0.0, Done: False\n",
      "Action: 4, Reward: 0.0, Done: False\n",
      "Action: 6, Reward: 0.0, Done: False\n",
      "Action: 2, Reward: 0.0, Done: False\n",
      "Action: 0, Reward: 0.0, Done: False\n",
      "Action: 7, Reward: 0.0, Done: False\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Practical_Reinforcement_Learning\\chapter6\\DQN.ipynb Cell 2\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/DQN.ipynb#W1sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m state\u001b[39m=\u001b[39menv\u001b[39m.\u001b[39m_get_observation()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/DQN.ipynb#W1sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m action \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39msample()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/DQN.ipynb#W1sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m obs, reward, done, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/DQN.ipynb#W1sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAction: \u001b[39m\u001b[39m{\u001b[39;00maction\u001b[39m}\u001b[39;00m\u001b[39m, Reward: \u001b[39m\u001b[39m{\u001b[39;00mreward\u001b[39m}\u001b[39;00m\u001b[39m, Done: \u001b[39m\u001b[39m{\u001b[39;00mdone\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32md:\\Practical_Reinforcement_Learning\\chapter6\\DQN.ipynb Cell 2\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/DQN.ipynb#W1sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/DQN.ipynb#W1sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     \u001b[39m# Take action and return next state, reward, done, and info\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/DQN.ipynb#W1sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     obs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_observation()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/DQN.ipynb#W1sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     reward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_calculate_reward(action)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/DQN.ipynb#W1sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     done \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_index \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[1;32md:\\Practical_Reinforcement_Learning\\chapter6\\DQN.ipynb Cell 2\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/DQN.ipynb#W1sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_observation\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/DQN.ipynb#W1sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     \u001b[39m# Get the current observation (image)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/DQN.ipynb#W1sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     image, _ \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/DQN.ipynb#W1sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m image\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[1;32md:\\Practical_Reinforcement_Learning\\.conda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:438\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    436\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator\n\u001b[0;32m    437\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 438\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_iterator()\n",
      "File \u001b[1;32md:\\Practical_Reinforcement_Learning\\.conda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:386\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    385\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 386\u001b[0m     \u001b[39mreturn\u001b[39;00m _MultiProcessingDataLoaderIter(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[1;32md:\\Practical_Reinforcement_Learning\\.conda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1039\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1032\u001b[0m w\u001b[39m.\u001b[39mdaemon \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1033\u001b[0m \u001b[39m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1034\u001b[0m \u001b[39m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[39m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m \u001b[39m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m \u001b[39m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m \u001b[39m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1039\u001b[0m w\u001b[39m.\u001b[39;49mstart()\n\u001b[0;32m   1040\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_queues\u001b[39m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1041\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_workers\u001b[39m.\u001b[39mappend(w)\n",
      "File \u001b[1;32md:\\Practical_Reinforcement_Learning\\.conda\\Lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m _current_process\u001b[39m.\u001b[39m_config\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdaemon\u001b[39m\u001b[39m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mdaemonic processes are not allowed to have children\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_Popen(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    122\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sentinel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen\u001b[39m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[39m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[39m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32md:\\Practical_Reinforcement_Learning\\.conda\\Lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_context\u001b[39m.\u001b[39;49mget_context()\u001b[39m.\u001b[39;49mProcess\u001b[39m.\u001b[39;49m_Popen(process_obj)\n",
      "File \u001b[1;32md:\\Practical_Reinforcement_Learning\\.conda\\Lib\\multiprocessing\\context.py:336\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    334\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    335\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpopen_spawn_win32\u001b[39;00m \u001b[39mimport\u001b[39;00m Popen\n\u001b[1;32m--> 336\u001b[0m     \u001b[39mreturn\u001b[39;00m Popen(process_obj)\n",
      "File \u001b[1;32md:\\Practical_Reinforcement_Learning\\.conda\\Lib\\multiprocessing\\popen_spawn_win32.py:94\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     reduction\u001b[39m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 94\u001b[0m     reduction\u001b[39m.\u001b[39;49mdump(process_obj, to_child)\n\u001b[0;32m     95\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     96\u001b[0m     set_spawning_popen(\u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32md:\\Practical_Reinforcement_Learning\\.conda\\Lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdump\u001b[39m(obj, file, protocol\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     ForkingPickler(file, protocol)\u001b[39m.\u001b[39;49mdump(obj)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "class CIFAR10Env(gym.Env):\n",
    "    def __init__(self, subset='train'):\n",
    "        super(CIFAR10Env, self).__init__()\n",
    "\n",
    "        # Load CIFAR-10 dataset\n",
    "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "        self.dataset = datasets.CIFAR10(root='./data', train=(subset == 'train'), download=True, transform=self.transform)\n",
    "        self.loader = torch.utils.data.DataLoader(self.dataset, batch_size=1, shuffle=True, num_workers=2)\n",
    "\n",
    "        # Define action and observation spaces\n",
    "        self.action_space = spaces.Discrete(10)  # 10 classes in CIFAR-10\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(3, 32, 32), dtype=np.float32)\n",
    "\n",
    "        # Initialize state\n",
    "        self.current_index = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_index = 0\n",
    "        return self._get_observation()\n",
    "\n",
    "    def step(self, action):\n",
    "        # Take action and return next state, reward, done, and info\n",
    "        obs = self._get_observation()\n",
    "        reward = self._calculate_reward(action)\n",
    "        done = self.current_index == len(self.dataset) - 1\n",
    "        info = {}\n",
    "\n",
    "        self.current_index += 1\n",
    "\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def _get_observation(self):\n",
    "        # Get the current observation (image)\n",
    "        image, _ = next(iter(self.loader))\n",
    "        return image.squeeze(0).numpy()\n",
    "\n",
    "    def _calculate_reward(self, action):\n",
    "        # Placeholder reward function (you may want to customize this based on your task)\n",
    "        true_label = self.dataset[self.current_index][1]\n",
    "        temp_reward = 1.0 if action == true_label else 0.0\n",
    "        \n",
    "        return float(temp_reward)\n",
    "\n",
    "# Example of using the CIFAR10Env\n",
    "env = CIFAR10Env(subset='train')\n",
    "\n",
    "# Reset the environment\n",
    "obs = env.reset()\n",
    "\n",
    "# Sample random actions for 10 steps\n",
    "for _ in range(10):\n",
    "    state=env._get_observation()\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    print(f\"Action: {action}, Reward: {reward}, Done: {done}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "import ptan\n",
    "import numpy as np\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_channels, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        # Calculate the size of the output after convolutional layers\n",
    "        self.fc_input_size = self._calculate_conv_output_size(input_channels, 16, 2) * \\\n",
    "                             self._calculate_conv_output_size(16, 32, 2) * \\\n",
    "                             self._calculate_conv_output_size(32, 64, 2) * \\\n",
    "                             self._calculate_conv_output_size(64, 128, 2)\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            # nn.Linear(2048, 2048),  # Update input size based on flattened output size\n",
    "            # nn.ReLU(),\n",
    "            nn.Linear(2048, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv_layers(x)\n",
    "        \n",
    "        # Print the shape of the output after convolutional layers\n",
    "        #print(\"Conv Output Shape:\", x.shape)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)  # Flatten the output for the fully connected layers\n",
    "        \n",
    "        # Print the shape of the flattened output\n",
    "        #print(\"Flattened Output Shape:\", x.shape)\n",
    "        \n",
    "        x = self.fc_layers(x)\n",
    "        \n",
    "        # Print the shape of the output after fully connected layers\n",
    "        #print(\"FC Output Shape:\", x.shape)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def _calculate_conv_output_size(self, in_channels, out_channels, stride):\n",
    "        # Function to calculate the size of the output after a convolutional layer\n",
    "        dummy_input = torch.zeros(1, in_channels, 32, 32)\n",
    "        dummy_output = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)(dummy_input)\n",
    "        return dummy_output.size(2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_batch(batch):\n",
    "    states, actions, rewards, dones, next_states = [], [], [], [], []\n",
    "    for experience in batch:\n",
    "        states.append(experience.state)\n",
    "        actions.append(experience.action)\n",
    "        rewards.append(experience.reward)\n",
    "        dones.append(experience[-1])  # Accessing done flag directly\n",
    "        next_states.append(experience.last_state)\n",
    "\n",
    "    states_v = torch.tensor(states, dtype=torch.float32)\n",
    "    actions_v = torch.tensor(actions)\n",
    "    rewards_v = torch.tensor(rewards, dtype=torch.float32)\n",
    "    \n",
    "    # Convert 'dones' to a NumPy array before creating the PyTorch tensor\n",
    "    dones_mask = torch.tensor(np.array(dones), dtype=torch.bool)\n",
    "    \n",
    "    next_states_v = torch.tensor(next_states, dtype=torch.float32)\n",
    "\n",
    "    return states_v, actions_v, rewards_v, dones_mask, next_states_v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (conv_layers): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc_layers): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=4096, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=4096, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = DQN(env.observation_space.shape[0], env.action_space.n)\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create CartPole environment\n",
    "env = CIFAR10Env(subset=\"train\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "#env.seed(42)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Neural network and optimizer\n",
    "net = DQN(env.observation_space.shape[0], env.action_space.n)\n",
    "target_net = ptan.agent.TargetNet(net)\n",
    "selector = ptan.actions.ArgmaxActionSelector()\n",
    "epsilon_greedy = ptan.actions.EpsilonGreedyActionSelector(epsilon=0.1, selector=selector)\n",
    "agent = ptan.agent.DQNAgent(net, epsilon_greedy, preprocessor=ptan.agent.float32_preprocessor)\n",
    "\n",
    "# Experience source\n",
    "exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=0.99, steps_count=1)\n",
    "\n",
    "# Experience buffer\n",
    "buffer = ptan.experience.ExperienceReplayBuffer(exp_source, buffer_size=1000)\n",
    "\n",
    "# Loss function and optimizer\n",
    "loss_func = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ptan.agent.DQNAgent at 0x1c04e586110>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_source.agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "#states_v, actions_v, rewards_v, dones_mask, next_states_v = unpack_batch(s)\n",
    "print(s)#states_v, actions_v, rewards_v, dones_mask, next_states_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv Output Shape: torch.Size([1, 128, 4, 4])\n",
      "Flattened Output Shape: torch.Size([1, 2048])\n",
      "FC Output Shape: torch.Size([1, 10])\n",
      "Conv Output Shape: torch.Size([1, 128, 4, 4])\n",
      "Flattened Output Shape: torch.Size([1, 2048])\n",
      "FC Output Shape: torch.Size([1, 10])\n",
      "Sample is None, buffer might not be ready.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if buffer.sample(1) is not None:\n",
    "    sample=buffer.populate(1)\n",
    "    # Check if the sample is None\n",
    "    if sample is not None:\n",
    "        states_v, actions_v, rewards_v, dones_mask, next_states_v = unpack_batch(sample)\n",
    "\n",
    "        # Other training steps...\n",
    "    else:\n",
    "        # Handle the case where the sample is None (e.g., buffer is not ready)\n",
    "        print(\"Sample is None, buffer might not be ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Practical_Reinforcement_Learning\\chapter6\\DQN.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/DQN.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m sample\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/DQN.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m states_v, actions_v, rewards_v, dones_mask, next_states_v \u001b[39m=\u001b[39m unpack_batch(sample)\n",
      "\u001b[1;32md:\\Practical_Reinforcement_Learning\\chapter6\\DQN.ipynb Cell 10\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/DQN.ipynb#X10sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39munpack_batch\u001b[39m(batch):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/DQN.ipynb#X10sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m     states, actions, rewards, dones, next_states \u001b[39m=\u001b[39m [], [], [], [], []\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/DQN.ipynb#X10sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m     \u001b[39mfor\u001b[39;49;00m experience \u001b[39min\u001b[39;49;00m batch:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/DQN.ipynb#X10sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m         states\u001b[39m.\u001b[39;49mappend(experience\u001b[39m.\u001b[39;49mstate)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/DQN.ipynb#X10sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m         actions\u001b[39m.\u001b[39;49mappend(experience\u001b[39m.\u001b[39;49maction)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "sample\n",
    "states_v, actions_v, rewards_v, dones_mask, next_states_v = unpack_batch(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ptan.agent.DQNAgent at 0x1c04e586110>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'buffer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Practical_Reinforcement_Learning\\chapter6\\DQN.ipynb Cell 13\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/DQN.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Training loop\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/DQN.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1000\u001b[39m):  \u001b[39m# You may want to adjust the number of steps\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/DQN.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     buffer\u001b[39m.\u001b[39mpopulate(\u001b[39m1\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/DQN.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m# Get batch from the buffer\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/DQN.ipynb#X15sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     batch \u001b[39m=\u001b[39m buffer\u001b[39m.\u001b[39msample(\u001b[39m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'buffer' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "best_reward = float('-inf')  # Initialize with negative infinity or other appropriate value\n",
    "best_model_path = \"best_model.pth\"  # Define the path where you want to save the best model\n",
    "checkpoint_interval = 5\n",
    "current_reward = 0.0\n",
    "# Training loop\n",
    "for step in range(1000):  # You may want to adjust the number of steps\n",
    "    buffer.populate(1)\n",
    "\n",
    "    # Get batch from the buffer\n",
    "    batch = buffer.sample(1)\n",
    "    states_v, actions_v, rewards_v, dones_mask, next_states_v = unpack_batch(batch)\n",
    "\n",
    "    # Zero gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    q_values = net(states_v)\n",
    "\n",
    "    # Get Q-values for taken actions\n",
    "        # Get target Q-values\n",
    "    target_q_values = target_net.target_model(next_states_v).max(1)[0]\n",
    "\n",
    "    # Use view(-1) to ensure dones_mask is 1-dimensional\n",
    "    dones_mask = dones_mask.view(-1)\n",
    "\n",
    "    # Ensure target_q_values is also 1-dimensional\n",
    "    target_q_values = target_q_values.view(-1)\n",
    "    print(target_q_values)\n",
    "    # # Update Q-values based on dones_mask\n",
    "    # target_q_values[dones_mask] = 0.0\n",
    "\n",
    "    # Detach target_q_values\n",
    "    target_q_values = target_q_values.detach()\n",
    "\n",
    "    # Calculate TD error\n",
    "    expected_q_values = rewards_v + target_q_values * 0.99\n",
    "    loss = loss_func(q_values, expected_q_values)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Optimize\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    if step % 10 == 0:\n",
    "        target_net.sync()\n",
    "    print(f\"\\n----------------------------Loss {loss.item()}------------\\n\")\n",
    "    if step % checkpoint_interval == 0:\n",
    "            # Check the performance and save the model if it's the best so far\n",
    "            if current_reward > best_reward:\n",
    "                best_reward = current_reward\n",
    "                torch.save(net.state_dict(), best_model_path)\n",
    "# After training, you can use the trained model for inference.\n",
    "# For example, you can run the trained agent in the environment:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv Output Shape: torch.Size([1, 128, 4, 4])\n",
      "Flattened Output Shape: torch.Size([1, 2048])\n",
      "FC Output Shape: torch.Size([1, 10])\n",
      "(array([7], dtype=int64), [None])\n",
      "Conv Output Shape: torch.Size([1, 128, 4, 4])\n",
      "Flattened Output Shape: torch.Size([1, 2048])\n",
      "FC Output Shape: torch.Size([1, 10])\n",
      "(array([3], dtype=int64), [None])\n",
      "Conv Output Shape: torch.Size([1, 128, 4, 4])\n",
      "Flattened Output Shape: torch.Size([1, 2048])\n",
      "FC Output Shape: torch.Size([1, 10])\n",
      "(array([3], dtype=int64), [None])\n",
      "Conv Output Shape: torch.Size([1, 128, 4, 4])\n",
      "Flattened Output Shape: torch.Size([1, 2048])\n",
      "FC Output Shape: torch.Size([1, 10])\n",
      "(array([3], dtype=int64), [None])\n",
      "Conv Output Shape: torch.Size([1, 128, 4, 4])\n",
      "Flattened Output Shape: torch.Size([1, 2048])\n",
      "FC Output Shape: torch.Size([1, 10])\n",
      "(array([7], dtype=int64), [None])\n",
      "Conv Output Shape: torch.Size([1, 128, 4, 4])\n",
      "Flattened Output Shape: torch.Size([1, 2048])\n",
      "FC Output Shape: torch.Size([1, 10])\n",
      "(array([3], dtype=int64), [None])\n",
      "Conv Output Shape: torch.Size([1, 128, 4, 4])\n",
      "Flattened Output Shape: torch.Size([1, 2048])\n",
      "FC Output Shape: torch.Size([1, 10])\n",
      "(array([3], dtype=int64), [None])\n",
      "Conv Output Shape: torch.Size([1, 128, 4, 4])\n",
      "Flattened Output Shape: torch.Size([1, 2048])\n",
      "FC Output Shape: torch.Size([1, 10])\n",
      "(array([3], dtype=int64), [None])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Practical_Reinforcement_Learning\\chapter6\\DQN.ipynb Cell 14\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/DQN.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m state \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/DQN.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/DQN.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m#env.render()\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/DQN.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     state \u001b[39m=\u001b[39menv\u001b[39m.\u001b[39;49m_get_observation() \u001b[39m# or numpy image\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/DQN.ipynb#X22sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m# Wrap the state in a batch-like structure\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/DQN.ipynb#X22sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     state_batch \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([state], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\n",
      "\u001b[1;32md:\\Practical_Reinforcement_Learning\\chapter6\\DQN.ipynb Cell 14\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/DQN.ipynb#X22sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_observation\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/DQN.ipynb#X22sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     \u001b[39m# Get the current observation (image)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/DQN.ipynb#X22sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     image, _ \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/DQN.ipynb#X22sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m image\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[1;32md:\\Practical_Reinforcement_Learning\\.conda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:438\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    436\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator\n\u001b[0;32m    437\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 438\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_iterator()\n",
      "File \u001b[1;32md:\\Practical_Reinforcement_Learning\\.conda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:386\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    385\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 386\u001b[0m     \u001b[39mreturn\u001b[39;00m _MultiProcessingDataLoaderIter(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[1;32md:\\Practical_Reinforcement_Learning\\.conda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1039\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1032\u001b[0m w\u001b[39m.\u001b[39mdaemon \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1033\u001b[0m \u001b[39m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1034\u001b[0m \u001b[39m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[39m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m \u001b[39m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m \u001b[39m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m \u001b[39m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1039\u001b[0m w\u001b[39m.\u001b[39;49mstart()\n\u001b[0;32m   1040\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_queues\u001b[39m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1041\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_workers\u001b[39m.\u001b[39mappend(w)\n",
      "File \u001b[1;32md:\\Practical_Reinforcement_Learning\\.conda\\Lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m _current_process\u001b[39m.\u001b[39m_config\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdaemon\u001b[39m\u001b[39m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mdaemonic processes are not allowed to have children\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_Popen(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    122\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sentinel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen\u001b[39m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[39m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[39m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32md:\\Practical_Reinforcement_Learning\\.conda\\Lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_context\u001b[39m.\u001b[39;49mget_context()\u001b[39m.\u001b[39;49mProcess\u001b[39m.\u001b[39;49m_Popen(process_obj)\n",
      "File \u001b[1;32md:\\Practical_Reinforcement_Learning\\.conda\\Lib\\multiprocessing\\context.py:336\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    334\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    335\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpopen_spawn_win32\u001b[39;00m \u001b[39mimport\u001b[39;00m Popen\n\u001b[1;32m--> 336\u001b[0m     \u001b[39mreturn\u001b[39;00m Popen(process_obj)\n",
      "File \u001b[1;32md:\\Practical_Reinforcement_Learning\\.conda\\Lib\\multiprocessing\\popen_spawn_win32.py:94\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     reduction\u001b[39m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 94\u001b[0m     reduction\u001b[39m.\u001b[39;49mdump(process_obj, to_child)\n\u001b[0;32m     95\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     96\u001b[0m     set_spawning_popen(\u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32md:\\Practical_Reinforcement_Learning\\.conda\\Lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdump\u001b[39m(obj, file, protocol\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     ForkingPickler(file, protocol)\u001b[39m.\u001b[39;49mdump(obj)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "while True:\n",
    "    #env.render()\n",
    "    state =env._get_observation() # or numpy image\n",
    "    # Wrap the state in a batch-like structure\n",
    "    state_batch = torch.tensor([state], dtype=torch.float32)\n",
    "    action= agent(state_batch)\n",
    "    print(action)\n",
    "    obs, reward, done, info= env.step(action)  # Extract the action value from the tensor\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'ptan' has no attribute 'ignite'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\Practical_Reinforcement_Learning\\chapter6\\DQN.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/DQN.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mDynamicPenaltyTracker\u001b[39;00m(ptan\u001b[39m.\u001b[39;49mignite\u001b[39m.\u001b[39mTrackable):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/DQN.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, initial_penalty\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, update_interval\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, update_factor\u001b[39m=\u001b[39m\u001b[39m0.9\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/DQN.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpenalty \u001b[39m=\u001b[39m initial_penalty\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'ptan' has no attribute 'ignite'"
     ]
    }
   ],
   "source": [
    "class DynamicPenaltyTracker(ptan.ignite.Trackable):\n",
    "    def __init__(self, initial_penalty=0.1, update_interval=100, update_factor=0.9):\n",
    "        self.penalty = initial_penalty\n",
    "        self.update_interval = update_interval\n",
    "        self.update_factor = update_factor\n",
    "        self.steps = 0\n",
    "\n",
    "    def frame(self, value):\n",
    "        self.steps += 1\n",
    "        if self.steps % self.update_interval == 0:\n",
    "            self.penalty *= self.update_factor\n",
    "        return self.penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DynamicPenaltyTracker' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Practical_Reinforcement_Learning\\chapter6\\DQN.ipynb Cell 16\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/DQN.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Create a DynamicPenaltyTracker\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/DQN.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m penalty_tracker \u001b[39m=\u001b[39m DynamicPenaltyTracker(initial_penalty\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, update_interval\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, update_factor\u001b[39m=\u001b[39m\u001b[39m0.9\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/DQN.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Training loop\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/DQN.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1000\u001b[39m):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DynamicPenaltyTracker' is not defined"
     ]
    }
   ],
   "source": [
    "# Create a DynamicPenaltyTracker\n",
    "penalty_tracker = DynamicPenaltyTracker(initial_penalty=0.1, update_interval=100, update_factor=0.9)\n",
    "\n",
    "# Training loop\n",
    "for step in range(1000):\n",
    "    buffer.populate(1)\n",
    "\n",
    "    # Get batch from the buffer\n",
    "    batch = buffer.sample(1)\n",
    "    states_v, actions_v, rewards_v, dones_mask, next_states_v = unpack_batch(batch)\n",
    "\n",
    "    # Forward pass\n",
    "    q_values = net(states_v)\n",
    "\n",
    "    # Get target Q-values\n",
    "    target_q_values = target_net.target_model(next_states_v).max(1)[0]\n",
    "    print(target_q_values)\n",
    "    target_q_values[dones_mask] = 0.0\n",
    "    target_q_values = target_q_values.detach()\n",
    "\n",
    "    # Calculate TD error\n",
    "    expected_q_values = rewards_v + target_q_values * 0.99\n",
    "    loss = loss_func(q_values, expected_q_values)\n",
    "\n",
    "    # Get the current penalty factor from the tracker\n",
    "    penalty_factor = penalty_tracker.frame(step)\n",
    "\n",
    "    # Apply penalty to the loss\n",
    "    loss += penalty_factor * (some_dynamic_value - some_target_value)\n",
    "\n",
    "    # Zero gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Optimize\n",
    "    optimizer.step()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
