{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'psutil' has no attribute 'Process'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification,AutoFeatureExtractor,AdamW\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import ViTImageProcessor, ViTModel\n",
    "from PIL import Image\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'psutil' has no attribute 'Process'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "!pip install psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "\n",
    "# Load pre-trained feature extractor\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "# Step 1: Modify classification head for the new number of classes\n",
    "num_classes = 10  # Update with your new number of classes\n",
    "model.classifier = torch.nn.Linear(model.config.hidden_size, num_classes)\n",
    "model.config.image_size=32\n",
    "# Move the model to the desired device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "class CustomVideoEnv(gym.Env):\n",
    "    def __init__(self, video_path):\n",
    "        super(CustomVideoEnv, self).__init__()\n",
    "\n",
    "        # Open the video file\n",
    "        self.video = cv2.VideoCapture(video_path)\n",
    "\n",
    "        # Get video properties\n",
    "        self.width = int(self.video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        self.height = int(self.video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "        # Set up observation space\n",
    "        self.observation_space = spaces.Box(low=0, high=255, shape=(self.height, self.width, 3), dtype=np.uint8)\n",
    "\n",
    "        # Set up action space (assuming discrete actions for simplicity)\n",
    "        self.action_space = spaces.Discrete(4)  # Four discrete actions: 0, 1, 2, 3\n",
    "\n",
    "        # Set up initial agent position\n",
    "        self.agent_pos = [0, 0]\n",
    "\n",
    "        # Read the first frame\n",
    "        _, self.current_frame = self.video.read()\n",
    "\n",
    "        # Set up the viewer (optional, for rendering)\n",
    "        self.viewer = None\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset agent position to the initial state\n",
    "        self.agent_pos = [0, 0]\n",
    "\n",
    "        # Read the first frame\n",
    "        _, self.current_frame = self.video.read()\n",
    "\n",
    "        # Return the initial observation\n",
    "        return self._get_observation()\n",
    "\n",
    "    def step(self, action):\n",
    "        # Move the agent based on the selected action\n",
    "        if action == 0:  # Move right\n",
    "            self.agent_pos[0] = min(self.agent_pos[0] + 1, self.width - 1)\n",
    "        elif action == 1:  # Move left\n",
    "            self.agent_pos[0] = max(self.agent_pos[0] - 1, 0)\n",
    "        elif action == 2:  # Move down\n",
    "            self.agent_pos[1] = min(self.agent_pos[1] + 1, self.height - 1)\n",
    "        elif action == 3:  # Move up\n",
    "            self.agent_pos[1] = max(self.agent_pos[1] - 1, 0)\n",
    "\n",
    "        # Set the next frame as the new observation\n",
    "        _, next_frame = self.video.read()\n",
    "\n",
    "        # Calculate the reward (e.g., negative distance to the target)\n",
    "        reward = -np.linalg.norm(np.array(self.agent_pos) - np.array([self.width // 2, self.height // 2]))\n",
    "\n",
    "        # Check if the agent has reached the center (for example)\n",
    "        done = (self.agent_pos == [self.width // 2, self.height // 2]).all()\n",
    "\n",
    "        # Update the current frame\n",
    "        self.current_frame = next_frame\n",
    "\n",
    "        # Return the observation, reward, done flag, and additional info (optional)\n",
    "        return self._get_observation(), reward, done, {}\n",
    "\n",
    "    def _get_observation(self):\n",
    "        # Resize the current frame to match the observation space\n",
    "        observation = cv2.resize(self.current_frame, (self.width, self.height))\n",
    "        return observation\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        # Render the environment (optional)\n",
    "        if self.viewer is None:\n",
    "            from gym.envs.classic_control import rendering\n",
    "            self.viewer = rendering.SimpleImageViewer()\n",
    "\n",
    "        self.viewer.imshow(self._get_observation())\n",
    "        return self.viewer.isopen\n",
    "\n",
    "    def close(self):\n",
    "        # Close the viewer (optional)\n",
    "        if self.viewer is not None:\n",
    "            self.viewer.close()\n",
    "        # Release the video capture object\n",
    "        self.video.release()\n",
    "\n",
    "# Example usage:\n",
    "video_path = 'path/to/your/video.mp4'\n",
    "env = CustomVideoEnv(video_path)\n",
    "\n",
    "# Reset the environment\n",
    "observation = env.reset()\n",
    "\n",
    "# Perform some random actions\n",
    "for _ in range(10):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, _ = env.step(action)\n",
    "    env.render()\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import ptan\n",
    "\n",
    "# Assuming you have defined CustomVideoEnv\n",
    "class CustomVideoEnv:\n",
    "    # ... (implementation details)\n",
    "\n",
    "# Create an instance of CustomVideoEnv (replace arguments as needed)\n",
    "custom_env = CustomVideoEnv(video_path='path/to/your/video.mp4')\n",
    "\n",
    "\n",
    "\n",
    "# Your additional training logic here\n",
    "class DullModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DullModel, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "        # Set weights and biases to make the model \"dull\"\n",
    "        self.fc.weight.data.fill_(0.0)\n",
    "        self.fc.bias.data.fill_(0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "class DullAgent:\n",
    "    def __init__(self, model, action_space):\n",
    "        self.model = model\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def select_action(self, state):\n",
    "        q_values = self.model(torch.FloatTensor(state).unsqueeze(0))\n",
    "        action = np.argmax(q_values.detach().numpy())\n",
    "        return action,q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a dull neural network model\n",
    "dull_model = DullModel(custom_env.observation_space.shape[0], custom_env.action_space.n)\n",
    "\n",
    "# Create a DullAgent\n",
    "dull_agent = DullAgent(dull_model, action_space=custom_env.action_space.n)\n",
    "\n",
    "# Define an optimizer\n",
    "optimizer = optim.Adam(dull_model.parameters(), lr=1e-3)\n",
    "# Training loop using ExperienceSourceFirstLast\n",
    "GAMMA = 0.99\n",
    "MAX_STEPS = 1000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Define the experience source\n",
    "exp_source = ptan.experience.ExperienceSourceFirstLast(custom_env, dull_agent, gamma=GAMMA, steps_count=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training loop (just an example, doesn't perform actual training with a dull model)\n",
    "for _ in range(1000):\n",
    "    # Generate a fake experience tuple\n",
    "    state = np.random.random(size=(input_size,))\n",
    "    action = dull_agent.select_action(state)\n",
    "    next_state = np.random.random(size=(input_size,))\n",
    "    reward = 0.0\n",
    "    done = False\n",
    "\n",
    "    experience = ptan.experience.ExperienceFirstLast(state, action, reward, last_state=next_state, done=done)\n",
    "\n",
    "    # Update the dull model (not really updating in this example)\n",
    "    optimizer.zero_grad()\n",
    "    loss = torch.nn.functional.mse_loss(dull_model(torch.FloatTensor(state).unsqueeze(0)), torch.zeros(1))\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for step, exp in enumerate(exp_source):\n",
    "    if step > MAX_STEPS:\n",
    "        break\n",
    "\n",
    "    # Process the experience tuple\n",
    "    states, actions, rewards, dones, next_states = exp\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    states_v = torch.FloatTensor(states)\n",
    "    actions_t = torch.LongTensor(actions)\n",
    "    rewards_v = torch.FloatTensor(rewards)\n",
    "    dones_mask = torch.BoolTensor(dones)\n",
    "    next_states_v = torch.FloatTensor(next_states)\n",
    "\n",
    "    # Compute Q-values for the current and next states\n",
    "    q_values = dull_model(states_v)\n",
    "    q_values_for_actions = q_values[range(len(actions)), actions_t]\n",
    "\n",
    "    # Compute Q-values for the next states\n",
    "    next_q_values = dull_model(next_states_v)\n",
    "    next_q_values[dones_mask] = 0.0  # Zero out Q-values for terminal states\n",
    "\n",
    "    # Compute the target Q-values using the Bellman equation\n",
    "    expected_q_values = rewards_v + GAMMA * next_q_values.max(dim=1)[0]\n",
    "\n",
    "    # Calculate the loss\n",
    "    loss = torch.nn.functional.mse_loss(q_values_for_actions, expected_q_values.detach())\n",
    "\n",
    "    # Perform the optimization step\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Your additional training logic here\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
