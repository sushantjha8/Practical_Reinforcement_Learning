{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Practical_Reinforcement_Learning\\.conda\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gym'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32md:\\Practical_Reinforcement_Learning\\chapter6\\main23.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/main23.ipynb#W0sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m ViTImageProcessor, ViTModel\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/main23.ipynb#W0sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m \u001b[39mimport\u001b[39;00m Image\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/main23.ipynb#W0sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgym\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/main23.ipynb#W0sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/main23.ipynb#W0sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gym'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification,AutoFeatureExtractor,AdamW\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import ViTImageProcessor, ViTModel\n",
    "from PIL import Image\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in d:\\practical_reinforcement_learning\\.conda\\lib\\site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in d:\\practical_reinforcement_learning\\.conda\\lib\\site-packages (from gym) (1.26.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in d:\\practical_reinforcement_learning\\.conda\\lib\\site-packages (from gym) (3.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in d:\\practical_reinforcement_learning\\.conda\\lib\\site-packages (from gym) (0.0.8)\n",
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.8.1.78-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in d:\\practical_reinforcement_learning\\.conda\\lib\\site-packages (from opencv-python) (1.26.0)\n",
      "Downloading opencv_python-4.8.1.78-cp37-abi3-win_amd64.whl (38.1 MB)\n",
      "   ---------------------------------------- 0.0/38.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.5/38.1 MB 10.0 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 1.8/38.1 MB 18.7 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 3.3/38.1 MB 23.3 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 3.6/38.1 MB 25.3 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 5.4/38.1 MB 23.0 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 7.1/38.1 MB 25.4 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 9.3/38.1 MB 28.3 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 11.4/38.1 MB 34.4 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 12.7/38.1 MB 32.8 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 14.0/38.1 MB 38.6 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 15.1/38.1 MB 32.8 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 16.4/38.1 MB 34.4 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 17.7/38.1 MB 32.7 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 19.1/38.1 MB 31.2 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 20.4/38.1 MB 29.7 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 21.6/38.1 MB 27.3 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 22.9/38.1 MB 27.3 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 24.3/38.1 MB 27.3 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 24.9/38.1 MB 28.5 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 25.1/38.1 MB 23.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 25.1/38.1 MB 21.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 26.2/38.1 MB 21.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 27.3/38.1 MB 20.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 28.4/38.1 MB 20.5 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 29.4/38.1 MB 19.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 30.7/38.1 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 31.7/38.1 MB 19.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 32.8/38.1 MB 19.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 33.9/38.1 MB 18.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 34.9/38.1 MB 18.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 35.9/38.1 MB 23.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 36.9/38.1 MB 22.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.0/38.1 MB 22.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 38.1/38.1 MB 21.1 MB/s eta 0:00:00\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.8.1.78\n"
     ]
    }
   ],
   "source": [
    "!pip install gym\n",
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "d:\\Practical_Reinforcement_Learning\\.conda\\Lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "\n",
    "# Load pre-trained feature extractor\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "# Step 1: Modify classification head for the new number of classes\n",
    "num_classes = 10  # Update with your new number of classes\n",
    "model.classifier = torch.nn.Linear(model.config.hidden_size, num_classes)\n",
    "model.config.image_size=32\n",
    "# Move the model to the desired device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "class CustomVideoEnv(gym.Env):\n",
    "    def __init__(self, video_path):\n",
    "        super(CustomVideoEnv, self).__init__()\n",
    "\n",
    "        # Open the video file\n",
    "        self.video = cv2.VideoCapture(video_path)\n",
    "\n",
    "        # Get video properties\n",
    "        self.width = int(self.video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        self.height = int(self.video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "        # Set up observation space\n",
    "        self.observation_space = spaces.Box(low=0, high=255, shape=(self.height, self.width, 3), dtype=np.uint8)\n",
    "\n",
    "        # Set up action space (assuming discrete actions for simplicity)\n",
    "        self.action_space = spaces.Discrete(4)  # Four discrete actions: 0, 1, 2, 3\n",
    "\n",
    "        # Set up initial agent position\n",
    "        self.agent_pos = [0, 0]\n",
    "\n",
    "        # Read the first frame\n",
    "        _, self.current_frame = self.video.read()\n",
    "\n",
    "        # Set up the viewer (optional, for rendering)\n",
    "        self.viewer = None\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset agent position to the initial state\n",
    "        self.agent_pos = [0, 0]\n",
    "\n",
    "        # Read the first frame\n",
    "        _, self.current_frame = self.video.read()\n",
    "\n",
    "        # Return the initial observation\n",
    "        return self._get_observation()\n",
    "\n",
    "    def step(self, action):\n",
    "        # Move the agent based on the selected action\n",
    "        if action == 0:  # Move right\n",
    "            self.agent_pos[0] = min(self.agent_pos[0] + 1, self.width - 1)\n",
    "        elif action == 1:  # Move left\n",
    "            self.agent_pos[0] = max(self.agent_pos[0] - 1, 0)\n",
    "        elif action == 2:  # Move down\n",
    "            self.agent_pos[1] = min(self.agent_pos[1] + 1, self.height - 1)\n",
    "        elif action == 3:  # Move up\n",
    "            self.agent_pos[1] = max(self.agent_pos[1] - 1, 0)\n",
    "\n",
    "        # Set the next frame as the new observation\n",
    "        _, next_frame = self.video.read()\n",
    "\n",
    "        # Calculate the reward (e.g., negative distance to the target)\n",
    "        reward = -np.linalg.norm(np.array(self.agent_pos) - np.array([self.width // 2, self.height // 2]))\n",
    "\n",
    "        # Check if the agent has reached the center (for example)\n",
    "        done = (self.agent_pos == [self.width // 2, self.height // 2]).all()\n",
    "\n",
    "        # Update the current frame\n",
    "        self.current_frame = next_frame\n",
    "\n",
    "        # Return the observation, reward, done flag, and additional info (optional)\n",
    "        return self._get_observation(), reward, done, {}\n",
    "\n",
    "    def _get_observation(self):\n",
    "        # Resize the current frame to match the observation space\n",
    "        observation = cv2.resize(self.current_frame, (self.width, self.height))\n",
    "        return observation\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        # Render the environment (optional)\n",
    "        if self.viewer is None:\n",
    "            from gym.envs.classic_control import rendering\n",
    "            self.viewer = rendering.SimpleImageViewer()\n",
    "\n",
    "        self.viewer.imshow(self._get_observation())\n",
    "        return self.viewer.isopen\n",
    "\n",
    "    def close(self):\n",
    "        # Close the viewer (optional)\n",
    "        if self.viewer is not None:\n",
    "            self.viewer.close()\n",
    "        # Release the video capture object\n",
    "        self.video.release()\n",
    "\n",
    "# Example usage:\n",
    "video_path = 'path/to/your/video.mp4'\n",
    "env = CustomVideoEnv(video_path)\n",
    "\n",
    "# # Reset the environment\n",
    "# observation = env.reset()\n",
    "\n",
    "# # Perform some random actions\n",
    "# for _ in range(10):\n",
    "#     action = env.action_space.sample()\n",
    "#     observation, reward, done, _ = env.step(action)\n",
    "#     env.render()\n",
    "\n",
    "# env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import ptan\n",
    "\n",
    "\n",
    "# Create an instance of CustomVideoEnv (replace arguments as needed)\n",
    "custom_env = CustomVideoEnv(video_path='path/to/your/video.mp4')\n",
    "\n",
    "\n",
    "\n",
    "# Your additional training logic here\n",
    "class DullModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DullModel, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "        # Set weights and biases to make the model \"dull\"\n",
    "        self.fc.weight.data.fill_(0.0)\n",
    "        self.fc.bias.data.fill_(0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "class DullAgent:\n",
    "    def __init__(self, model, action_space):\n",
    "        self.model = model\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def select_action(self, state):\n",
    "        q_values = self.model(torch.FloatTensor(state).unsqueeze(0))\n",
    "        action = np.argmax(q_values.detach().numpy())\n",
    "        return action,q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a dull neural network model\n",
    "dull_model = DullModel(custom_env.observation_space.shape[0], custom_env.action_space.n)\n",
    "\n",
    "# Create a DullAgent\n",
    "dull_agent = DullAgent(dull_model, action_space=custom_env.action_space.n)\n",
    "\n",
    "# Define an optimizer\n",
    "optimizer = optim.Adam(dull_model.parameters(), lr=1e-3)\n",
    "# Training loop using ExperienceSourceFirstLast\n",
    "GAMMA = 0.99\n",
    "MAX_STEPS = 1000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the experience source for 1 step\n",
    "exp_source = ptan.experience.ExperienceSourceFirstLast(custom_env, dull_agent, gamma=GAMMA, steps_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training loop (just an example, doesn't perform actual training with a dull model)\n",
    "for _ in range(1000):\n",
    "    # Generate a fake experience tuple\n",
    "    state = np.random.random(size=(input_size,))\n",
    "    action = dull_agent.select_action(state)\n",
    "    next_state = np.random.random(size=(input_size,))\n",
    "    reward = 0.0\n",
    "    done = False\n",
    "\n",
    "    experience = ptan.experience.ExperienceFirstLast(state, action, reward, last_state=next_state, done=done)\n",
    "\n",
    "    # Update the dull model (not really updating in this example)\n",
    "    optimizer.zero_grad()\n",
    "    loss = torch.nn.functional.mse_loss(dull_model(torch.FloatTensor(state).unsqueeze(0)), torch.zeros(1))\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for step, exp in enumerate(exp_source):\n",
    "    if step > MAX_STEPS:\n",
    "        break\n",
    "\n",
    "    # Process the experience tuple\n",
    "    states, actions, rewards, dones, next_states = exp\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    states_v = torch.FloatTensor(states)\n",
    "    actions_t = torch.LongTensor(actions)\n",
    "    rewards_v = torch.FloatTensor(rewards)\n",
    "    dones_mask = torch.BoolTensor(dones)\n",
    "    next_states_v = torch.FloatTensor(next_states)\n",
    "\n",
    "    # Compute Q-values for the current and next states\n",
    "    q_values = dull_model(states_v)\n",
    "    q_values_for_actions = q_values[range(len(actions)), actions_t]\n",
    "\n",
    "    # Compute Q-values for the next states\n",
    "    next_q_values = dull_model(next_states_v)\n",
    "    next_q_values[dones_mask] = 0.0  # Zero out Q-values for terminal states\n",
    "\n",
    "    # Compute the target Q-values using the Bellman equation\n",
    "    expected_q_values = rewards_v + GAMMA * next_q_values.max(dim=1)[0]\n",
    "\n",
    "    # Calculate the loss\n",
    "    loss = torch.nn.functional.mse_loss(q_values_for_actions, expected_q_values.detach())\n",
    "\n",
    "    # Perform the optimization step\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Your additional training logic here\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
