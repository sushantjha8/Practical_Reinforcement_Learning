{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env\n",
    "#net\n",
    "#tnet intilaise all above sequentialy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification,AutoFeatureExtractor,AdamW\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import ViTImageProcessor, ViTModel\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Using GPU.\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    \n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "else:\n",
    "    \n",
    "    print(\"CUDA is not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\sushant\\anaconda3\\Lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "# inputs = processor(images=image, return_tensors=\"pt\")\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "# Step 2: Modify classification head for the new number of classes\n",
    "num_classes = 10  # Update with your new number of classes\n",
    "model.classifier = torch.nn.Linear(model.config.hidden_size, num_classes)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1cf0c2e83d0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import cifar10 dataset\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Download the training set\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Download the test set\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create DataLoader for training set\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "# Create DataLoader for test set\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)\n",
    "trainloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sushant\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create buffers frames (sample labeled data) which will be used by our agent.\n",
    "buffer_data = trainloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym.spaces\n",
    "from gym.utils import seeding\n",
    "from gym.envs.registration import EnvSpec\n",
    "import enum\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actions(enum.Enum):\n",
    "    Airplane = 0\n",
    "    Automobile = 1\n",
    "    Bird = 2\n",
    "    Cat = 3\n",
    "    Deer = 4\n",
    "    Dog = 5\n",
    "    Frog = 6\n",
    "    Horse = 7\n",
    "    Ship = 8\n",
    "    Truck = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class state:\n",
    "   def __init__(self):\n",
    "      pass\n",
    "   \n",
    "   def step(self,action,label):\n",
    "      assert isinstance(action, Actions)\n",
    "      reward = 0.0\n",
    "      done = False\n",
    "      close = self._cur_close()\n",
    "      if action == Actions.Buy and not self.have_position:\n",
    "         self.have_position = True\n",
    "         self.open_price = close\n",
    "         reward -= self.commission_perc\n",
    "      elif action == Actions.Close and self.have_position:\n",
    "         reward -= self.commission_perc\n",
    "         done |= self.reset_on_close\n",
    "         if self.reward_on_close:\n",
    "               reward += 100.0 * (close / self.open_price - 1.0)\n",
    "         self.have_position = False\n",
    "      self.open_price = 0.0\n",
    "      self._offset += 1\n",
    "      prev_close = close\n",
    "      close = self._cur_close()\n",
    "      done |= self._offset >= self._prices.close.shape[0]-1\n",
    "\n",
    "      if self.have_position and not self.reward_on_close:\n",
    "         reward += 100.0 * (close / prev_close - 1.0)\n",
    "\n",
    "      return self.observation,reward, done\n",
    "   def update(self, new_observation):\n",
    "        # Update the state with the new observation\n",
    "        self.observation = new_observation\n",
    "   def reset(self,observation):\n",
    "       self.observation = observation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self,net ,\n",
    "                 reset_on_close=True, state_1d=False,\n",
    "                 random_ofs_on_reset=True, reward_on_close=False,\n",
    "                 volumes=False):\n",
    "        super(CIFAR10, self).__init__()\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0, high=255,shape=(80, 80, 3), dtype=np.uint8)\n",
    "        self.net = net\n",
    "        self.action_space = gym.spaces.Discrete(n=len(Actions))\n",
    "        self.state = state()\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the environment\n",
    "        # Return initial observation\n",
    "        \n",
    "        return np.zeros((4,), dtype=np.float32)\n",
    "    \n",
    "\n",
    "    def update(self, new_observation):\n",
    "        # Update the state with the new observation\n",
    "        self.observation = new_observation\n",
    "\n",
    "\n",
    "    def step(self, action_idx,label):\n",
    "        # Take a step in the environment based on the given action\n",
    "        # Return new observation, reward, done, and info\n",
    "        self.state.update(self.observation_space)\n",
    "        action = Actions(action_idx)\n",
    "\n",
    "        reward, done = self.state.step(action,label)\n",
    "        info = {\n",
    "            \"instrument\": self._instrument,\n",
    "            \"offset\": self._state._offset\n",
    "        }\n",
    "        return reward, done, info\n",
    "    def set_observation(self,input_frame):\n",
    "        self.observation_space = input_frame\n",
    "\n",
    "    def _reset_environment(self):\n",
    "        # Implement your environment's reset logic here and return the initial observation\n",
    "        # This is a placeholder, replace it with your own environment's reset logic\n",
    "        initial_observation = np.random.randint(0, 256, size=(height, width, 3), dtype=np.uint8)\n",
    "        return initial_observation\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "env =CIFAR10(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initiate Agent with buffer_data\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self,buffer,model,optimizer):\n",
    "        self.experiance = buffer\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.env = env\n",
    "    \n",
    "    def _reset(self):\n",
    "        self.experiance = None\n",
    "\n",
    "    \n",
    "    def _pretrain(self,epoch=4):\n",
    "        for epoch in range(0,epoch):\n",
    "            model.train()\n",
    "            trainloader = self.experiance \n",
    "            # Perform your training/validation steps here\n",
    "            \n",
    "            # Use Python's iter and next functions to obtain a single batch\n",
    "            # batch_iterator = iter(trainloader)\n",
    "            # batchs = next(batch_iterator)\n",
    "            print(len(trainloader))\n",
    "            count = 0\n",
    "            for batch in tqdm(trainloader):\n",
    "                count += 1\n",
    "                #print(count)#len(batch),batch[0].size(),batch[1].size())\n",
    "                running_loss = 0.0\n",
    "            \n",
    "                inputs, labels = batch[0].to(device), batch[1].to(device)\n",
    "                #print(inputs.shape)\n",
    "\n",
    "                    # inputs = feature_extractor(inputs, return_tensors=\"pt\").to(device)\n",
    "                outputs = self.model(inputs)\n",
    "                    # print(outputs)\n",
    "                    # # Depending on the model version, use either logits or last_hidden_state\n",
    "                logits = outputs.logits if hasattr(outputs, \"logits\") else outputs.last_hidden_state\n",
    "                \n",
    "\n",
    "                # Compute loss\n",
    "                loss = torch.nn.functional.cross_entropy(logits, labels)\n",
    "                   \n",
    "                    # #loss = outputs.loss\n",
    "                running_loss += loss.item()\n",
    "            print(running_loss)\n",
    "            loss.backward()\n",
    "            optimizer.step() \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "        return model, running_loss\n",
    "    \n",
    "    \n",
    "    def _play_testset_episode(self,testloader):\n",
    "        #model.eval()\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "        with torch.no_grad():  # No need to compute gradients during testing\n",
    "            batch_iterator = iter(testloader)\n",
    "            batch = next(batch_iterator)\n",
    "            for batch in batch_iterator:\n",
    "                inputs, labels = batch[0].to(device), batch[1].to(device)\n",
    "                # Forward pass through the model\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # Assuming 'logits' is the key returned by your model\n",
    "                logits = outputs.logits if hasattr(outputs, \"logits\") else outputs.last_hidden_state\n",
    "\n",
    "                # Get the predicted class (index with the maximum probability)\n",
    "                _, predicted = torch.max(logits, 1)\n",
    "                print(predicted)\n",
    "                # Update accuracy statistics\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "                total_samples += labels.size(0)\n",
    "        return total_samples\n",
    "    def play_episode(self,action,epsilon):\n",
    "        done_reward = None\n",
    "\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        label = \"Bird\"\n",
    "        # do step in the environment\n",
    "        new_state, reward, is_done, _ = self.env.step(action,label)\n",
    "        self.total_reward += reward\n",
    "\n",
    "        exp = Experience(self.state, action, reward,\n",
    "                         is_done, new_state)\n",
    "        self.exp_buffer.append(exp)\n",
    "        self.state = new_state\n",
    "        if is_done:\n",
    "            done_reward = self.total_reward\n",
    "            self._reset()\n",
    "        return done_reward\n",
    "    @torch.no_grad()\n",
    "    def for_current_episode(self,model,path,epsilon):\n",
    "        # Step 2: Preprocess the image\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "        image_path = path\n",
    "        image = Image.open(image_path)\n",
    "        image_tensor = transform(image)\n",
    "        # image_tensor = (image_tensor - image_tensor.min()) / (image_tensor.max() - image_tensor.min())  # Normalize to [0, 1]\n",
    "\n",
    "        # Step 3: Forward pass\n",
    "        inputs = image_tensor.unsqueeze(0).to(device)  # Add batch dimension\n",
    "        self.env.set_observation(inputs)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Step 4: Post-process predictions\n",
    "        # Assuming 'logits' is the key returned by your model\n",
    "        logits = outputs.logits if hasattr(outputs, \"logits\") else outputs.last_hidden_state\n",
    "        probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
    "\n",
    "        # Get the predicted class\n",
    "        predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "        reward = self.play_episode(predicted_class,epsilon)\n",
    "\n",
    "        return reward,logits\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(trainloader,model,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, running_loss=agent._pretrain(epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "nxt_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(logits,logits_nextstate,gamma,rewards):\n",
    "    expected_value = logits_nextstate * gamma + rewards\n",
    "    #print(expected_value)\n",
    "    return torch.nn.MSELoss()(logits,expected_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Practical_Reinforcement_Learning\\chapter6\\DQN_basics.ipynb Cell 15\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/DQN_basics.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m running_loss\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/DQN_basics.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m,\u001b[39m60\u001b[39m):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/DQN_basics.ipynb#X20sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/DQN_basics.ipynb#X20sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Practical_Reinforcement_Learning/chapter6/DQN_basics.ipynb#X20sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     result_s,logits\u001b[39m=\u001b[39magent\u001b[39m.\u001b[39mfor_current_episode(model,\u001b[39m'\u001b[39m\u001b[39m./0000.jpg\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "limit = 0.26\n",
    "\n",
    "running_loss=0\n",
    "for i in range(0,60):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    result_s,logits=agent.for_current_episode(model,'./0000.jpg')\n",
    "    imgclass=0\n",
    "\n",
    "    result_nextstate,logits_nextstate = agent.for_next_episode(nxt_model,'./0002.jpg')\n",
    "    nextstateclass = 2\n",
    "    reward = np.random.rand()\n",
    "    \n",
    "    # logits.requires_grad = True\n",
    "    # logits_nextstate.requires_grad = True\n",
    "    \n",
    "    print(reward) \n",
    "    loss = custom_loss(logits, logits,0.9,reward)\n",
    "    loss.requires_grad = True\n",
    "    loss = custom_loss(logits, logits_nextstate,0.9,reward)\n",
    "    loss.requires_grad = True\n",
    "    print(loss)\n",
    "    running_loss += loss.item()\n",
    "        \n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    running_loss += loss.item()\n",
    "    print(f\"running loss {running_loss}\")\n",
    "    \n",
    "    #     if result_s is imgclass and result_nextstate is nextstateclass:\n",
    "    #         reward = reward+0.75\n",
    "    #     else:\n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Connect ENV with Agent to \n",
    "To do that we need to create envorainment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ptan gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Agent loss within two model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
