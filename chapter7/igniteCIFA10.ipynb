{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This Notebook defines training of model using ignite method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-y3EF0cpHbV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "ONctZpJAr1PC"
      },
      "outputs": [],
      "source": [
        "# !pip install pytorch-ignite\n",
        "# !pip install tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "PVrVpSFAr2qA"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from transformers import ViTFeatureExtractor, ViTForImageClassification,AutoFeatureExtractor,AdamW\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import ViTImageProcessor, ViTModel\n",
        "from PIL import Image\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "NiZ9nru4sAIO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA is available. Using GPU.\n"
          ]
        }
      ],
      "source": [
        "# Check if CUDA (GPU support) is available\n",
        "if torch.cuda.is_available():\n",
        "\n",
        "    print(\"CUDA is available. Using GPU.\")\n",
        "else:\n",
        "\n",
        "    print(\"CUDA is not available. Using CPU.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eALsUVvhsJRW"
      },
      "outputs": [],
      "source": [
        "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "\n",
        "# Load pre-trained feature extractor\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
        "\n",
        "# Step 1: Modify classification head for the new number of classes\n",
        "num_classes = 10  # Update with your new number of classes\n",
        "model.classifier = torch.nn.Linear(model.config.hidden_size, num_classes)\n",
        "model.config.image_size=32\n",
        "# Move the model to the desired device (GPU if available, otherwise CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "gM7NnAUosX4f"
      },
      "outputs": [],
      "source": [
        "index_to_label = {\n",
        "    0: 'Airplane',\n",
        "    1: 'Automobile',\n",
        "    2: 'Bird',\n",
        "    3: 'Cat',\n",
        "    4: 'Deer',\n",
        "    5: 'Dog',\n",
        "    6: 'Frog',\n",
        "    7: 'Horse',\n",
        "    8: 'Ship',\n",
        "    9: 'Truck'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "QjDmPU5DslGv"
      },
      "outputs": [],
      "source": [
        "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
        "from ignite.metrics import Accuracy,Loss\n",
        "from torch.utils.data import Subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "z4vxSFSGsplQ"
      },
      "outputs": [],
      "source": [
        "from ignite.handlers import ModelCheckpoint\n",
        "from ignite.contrib.handlers import TensorboardLogger, global_step_from_engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "HSR3xeK7sd4A"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# Create the dataset and data loaders\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# Choose the number of samples you want to keep\n",
            "num_train_samples = 50\n",
            "num_test_samples = 10\n",
            "\n",
            "# Create subsets with a smaller number of samples\n",
            "train_subset = Subset(train_dataset, list(range(num_train_samples)))\n",
            "test_subset = Subset(test_dataset, list(range(num_test_samples)))\n",
            "batch_size = 8\n",
            "\n",
            "# Create data loaders for the subsets\n",
            "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
            "test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False, num_workers=4)\n"
          ]
        }
      ],
      "source": [
        "print('''# Choose the number of samples you want to keep\n",
        "num_train_samples = 50\n",
        "num_test_samples = 10\n",
        "\n",
        "# Create subsets with a smaller number of samples\n",
        "train_subset = Subset(train_dataset, list(range(num_train_samples)))\n",
        "test_subset = Subset(test_dataset, list(range(num_test_samples)))\n",
        "batch_size = 8\n",
        "\n",
        "# Create data loaders for the subsets\n",
        "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False, num_workers=4)''')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = model  # Initialize your ViT model here\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def trainer_score_function(engine):\n",
        "    return engine.state.metrics['accuracy'] \n",
        "    #return engine.state.metrics['accuracy']\n",
        "val_metrics = {\n",
        "    \"accuracy\": Accuracy(),\n",
        "    \"loss\": Loss(criterion)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "x5vCdbUysqKa"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "from ignite.engine import Engine\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from ignite.engine import Events\n",
        "from ignite.metrics import Accuracy\n",
        "from ignite.contrib.handlers import TensorboardLogger, global_step_from_engine\n",
        "from ignite.contrib.handlers import ProgressBar\n",
        "tb_logger = TensorboardLogger(log_dir=\"tb-logger\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ignite.metrics import Precision, Recall,ConfusionMatrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [],
      "source": [
        "pbar = ProgressBar(persist=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_accuracy(predictions, targets):\n",
        "    # Get the predicted class indices\n",
        "    predicted_classes = predictions.argmax(dim=1)\n",
        "    \n",
        "    # Compare with the target classes\n",
        "    correct_predictions = (predicted_classes == targets).float()\n",
        "    \n",
        "    # Calculate accuracy\n",
        "    accuracy = correct_predictions.mean().item()\n",
        "    \n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "J9VC5mueszZW"
      },
      "outputs": [],
      "source": [
        "def train_step(engine, batch):\n",
        "    model.train()\n",
        "    inputs, targets = batch\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs.to(device))\n",
        "\n",
        "    logits = outputs.logits if hasattr(outputs, \"logits\") else outputs.last_hidden_state\n",
        "\n",
        "    probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
        "    \n",
        "    loss = criterion(probabilities, targets)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    # # Gradient Clipping\n",
        "    clip_grad_norm_(model.parameters(), max_norm=0.9)  # Adjust max_norm as needed\n",
        "    \n",
        "    return logits,targets\n",
        "\n",
        "\n",
        "\n",
        "# Create Ignite trainer and evaluator\n",
        "trainer = Engine(train_step)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Precision and Recall metrics\n",
        "precision = Precision(average=True)  # average can be 'binary', 'micro', 'macro', or 'weighted'\n",
        "recall = Recall(average=True)  # average can be 'binary', 'micro', 'macro', or 'weighted'\n",
        "# Create the ConfusionMatrix metric\n",
        "confusion_matrix = ConfusionMatrix(num_classes=10)\n",
        "\n",
        "# Attach metrics to evaluator engine\n",
        "precision.attach(trainer, \"precision\")\n",
        "recall.attach(trainer, \"recall\")\n",
        "Accuracy().attach(trainer, \"accuracy\")\n",
        "# Attach the metric to the evaluator engine\n",
        "confusion_matrix.attach(trainer, \"confusion_matrix\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_checkpoint = ModelCheckpoint(\n",
        "    \"checkpoint\",\n",
        "    n_saved=2,\n",
        "    filename_prefix=\"best\",\n",
        "    score_function=trainer_score_function,\n",
        "    score_name=\"accuracy\",\n",
        "    global_step_transform=global_step_from_engine(trainer),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# @trainer.on(Events.EPOCH_COMPLETED)\n",
        "# def log_training_results(engine):\n",
        "#     metrics = trainer.state.metrics\n",
        "    #print(metrics)\n",
        "    # logits=engine.state.output[0].item()\n",
        "    # probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
        "    \n",
        "    # loss = criterion(probabilities, engine.state.output[1])\n",
        "     # Log metrics to TensorBoard\n",
        "    #global_step = engine.state.iteration\n",
        "    # tb_logger.add_scalar(\"Loss\", loss, global_step)\n",
        "    #\n",
        "\n",
        "# @trainer.on(Events.EPOCH_COMPLETED)\n",
        "# def log_validation_results(evaluator):\n",
        "#     evaluator.run(test_loader)\n",
        "#     metrics = evaluator.state.metrics\n",
        "#     print(f\"Validation Results - Epoch[{evaluator.state.epoch}] Avg accuracy: {metrics['accuracy']:.2f} Avg loss: {metrics['loss']:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets declare evaluator "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [],
      "source": [
        "def validation_step(engine, batch):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    with torch.no_grad():\n",
        "        x, y= batch\n",
        "        y = y.to(device)\n",
        "        #probabilities = torch.nn.functional.softmax(model(x.to(device)).logits, dim=1)\n",
        "        y_pred = model(x.to(device))\n",
        "        \n",
        "\n",
        "    return y_pred.logits, y # Include ground truth labels\n",
        "evaluator = Engine(validation_step)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Attach metrics to evaluator engine\n",
        "precision.attach(evaluator, \"precision\")\n",
        "recall.attach(evaluator, \"recall\")\n",
        "Accuracy().attach(evaluator, \"accuracy\")\n",
        "# Attach the metric to the evaluator engine\n",
        "confusion_matrix.attach(evaluator, \"confusion_matrix\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<ignite.engine.events.RemovableEventHandle at 0x2e0f70bc610>"
            ]
          },
          "execution_count": 111,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reward_window = []\n",
        "best_accuracy = 0.0\n",
        "@trainer.on(Events.EPOCH_COMPLETED)\n",
        "def log_validation_results(engine):\n",
        "    evaluator.run(test_loader)\n",
        "    metrics = evaluator.state.metrics\n",
        "    print(f\"Validation Results - Epoch[{evaluator.state.epoch}] Avg accuracy: {metrics['accuracy']:.2f}\")\n",
        "\n",
        "    \n",
        "@trainer.on(Events.EPOCH_COMPLETED)\n",
        "def update_with_reward(evaluator):\n",
        "    print(\"done\")\n",
        "    global best_accuracy\n",
        "    # Do not run the evaluator here, as it's already run in log_validation_results\n",
        "    # Use the validation accuracy as the reward from evaluator.state.metrics\n",
        "    reward = evaluator.state.metrics['accuracy']\n",
        "\n",
        "    # Update the model based on the reward\n",
        "    if reward > best_accuracy:\n",
        "        best_accuracy = reward\n",
        "        reward_window.append(reward)\n",
        "        print(f\"Updating model with reward: {reward}\")\n",
        "        torch.save(model.state_dict(), \"best_model.pth\")\n",
        "\n",
        "\n",
        "\n",
        "# Save the model after every epoch of val_evaluator is completed\n",
        "evaluator.add_event_handler(Events.COMPLETED, model_checkpoint, {\"model\": model})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Attach TensorBoard logger for loss, accuracy, and AUC\n",
        "tb_logger.attach_output_handler(\n",
        "    trainer,\n",
        "    event_name=Events.ITERATION_COMPLETED(every=100),\n",
        "    tag=\"training\",\n",
        "    output_transform=lambda loss: {\"batch_loss\": loss},\n",
        ")\n",
        "\n",
        "tb_logger.attach_output_handler(\n",
        "    evaluator,\n",
        "    event_name=Events.EPOCH_COMPLETED,\n",
        "    tag=\"validation\",\n",
        "    metric_names=['loss', 'accuracy'],\n",
        "    global_step_transform=global_step_from_engine(trainer),\n",
        ")\n",
        "for tag, evaluator in [(\"training\", trainer), (\"validation\", evaluator)]:\n",
        "    tb_logger.attach_output_handler(\n",
        "        evaluator,\n",
        "        event_name=Events.EPOCH_COMPLETED,\n",
        "        tag=tag,\n",
        "        metric_names=\"all\",\n",
        "        global_step_transform=global_step_from_engine(trainer),\n",
        "    )\n",
        "# Attach TensorBoard logger to the evaluator for validation metrics, including loss\n",
        "for name, metric in val_metrics.items():\n",
        "    tb_logger.attach_output_handler(\n",
        "        evaluator,\n",
        "        event_name=Events.EPOCH_COMPLETED,\n",
        "        tag=\"validation\",\n",
        "        metric_names=[name],\n",
        "        global_step_transform=global_step_from_engine(trainer),\n",
        "    )\n",
        "pbar.attach(trainer, metric_names=['loss'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.run(train_loader, max_epochs=6)\n",
        "# evaluator.run(test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tb_logger.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "trainer vs. evaluator: In Ignite, the `trainer` refers to the engine responsible for the training loop, while the `evaluator` refers to the engine responsible for evaluation (validation or testing). They are separate engines with distinct roles.\n",
        "\n",
        "The trainer is responsible for training the model using the training data loader and the specified training logic (train_step function).\n",
        "The evaluator is responsible for evaluating the model using the validation or test data loader and the specified evaluation logic (validation_step function)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Iginte Event Management\n",
        "\n",
        "* Events.STARTED: Triggered when the engine is started.\n",
        "* Events.COMPLETED: Triggered when the engine is completed.\n",
        "* Events.EPOCH_STARTED: Triggered at the beginning of each epoch.\n",
        "* Events.EPOCH_COMPLETED: Triggered at the end of each epoch.\n",
        "* Events.ITERATION_STARTED: Triggered at the beginning of each iteration (batch).\n",
        "* Events.ITERATION_COMPLETED: Triggered at the end of each iteration (batch).\n",
        "* Events.EXCEPTION_RAISED: Triggered when an exception is raised in the engine.\n",
        "* Events.TERMINATE: Triggered when the engine should terminate.\n",
        "* Events.MODEL_CHECKPOINT: Triggered when a model checkpoint is about to be saved.\n",
        "* Events.REDUCE_LR_ON_PLATEAU: Triggered during the learning rate reduction on plateau."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K37HZKmbXyaM"
      },
      "outputs": [],
      "source": [
        "!pip install jupyter-tensorboard\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUA3gdAfYY91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SyWRCmYq1-KL"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir=./tb-logger"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
